{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1. Transform data to record format\n",
    "## First dataset from http://www.robots.ox.ac.uk/~vgg/data/scenetext/\n",
    "## This method failed, because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import gzip\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'../processing/')\n",
    "from datasets import sythtextprovider\n",
    "import tensorflow as tf\n",
    "import skimage.io as skio\n",
    "#tf.InteractiveSession()\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "slim = tf.contrib.slim\n",
    "tf.__version__\n",
    "#from image_processing2 import *\n",
    "from processing import ssd_vgg_preprocessing\n",
    "import tf_extended as tfe\n",
    "from processing import tf_image\n",
    "from nets import txtbox_300\n",
    "import tf_utils\n",
    "from nets import custom_layers\n",
    "import load_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ffe377b962d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msyntext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/sythtext/gt.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msyntext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/python/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mMR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mmdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatfile_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/python/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m/Applications/python/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    250\u001b[0m            \u001b[0;34m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         '''\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "syntext = sio.loadmat('../data/sythtext/gt.mat')\n",
    "syntext.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordBB = syntext['wordBB']\n",
    "imnames = syntext['imnames']\n",
    "txt = syntext['txt']\n",
    "print imnames[0,10][0]\n",
    "index = 10\n",
    "img = cv2.imread(imnames[0,index][0])\n",
    "bbox = wordBB[0,index]\n",
    "text = txt[0,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bbox[:,:,0]\n",
    "np.min(bbox[:,:,0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bboxes):\n",
    "    \"\"\"\n",
    "    Input: image (height, width, channels)\n",
    "           bboxes (numof bboxes, 4) in order(ymin, xmin, ymax, xmax)\n",
    "                  range(0,1) \n",
    "    \"\"\"\n",
    "    numofbox = bboxes.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "    def norm(x):\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        else:\n",
    "            if x > 1:\n",
    "                x = 1\n",
    "        return x\n",
    "    xmin = [int(i * width) for i in bboxes[:,1]]\n",
    "    ymin = [int(i * height) for i in bboxes[:,0]]\n",
    "    ymax = [int(i * height) for i in bboxes[:,2]]\n",
    "    xmax = [int(i * width) for i in bboxes[:,3]]\n",
    "\n",
    "    for i in range(numofbox):\n",
    "        image = cv2.rectangle(image,(xmin[i],ymin[i]),\n",
    "                             (xmax[i],ymax[i]),(0,255,255))\n",
    "    print [ymin,xmin,ymax,xmax]\n",
    "    skio.imshow(image)\n",
    "    skio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_R_MEAN = 123.\n",
    "_G_MEAN = 117.\n",
    "_B_MEAN = 104.\n",
    "\n",
    "# Some training pre-processing parameters.\n",
    "BBOX_CROP_OVERLAP = 0.4        # Minimum overlap to keep a bbox after cropping.\n",
    "CROP_RATIO_RANGE = (0.8, 1.2)  # Distortion ratio during cropping.\n",
    "EVAL_SIZE = (300, 300)\n",
    "def tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n",
    "    \"\"\"Subtracts the given means from each image channel.\n",
    "\n",
    "    Returns:\n",
    "        the centered image.\n",
    "    \"\"\"\n",
    "    if image.get_shape().ndims != 3:\n",
    "        raise ValueError('Input must be of size [height, width, C>0]')\n",
    "    num_channels = image.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image - mean\n",
    "    return image\n",
    "\n",
    "\n",
    "def tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image + mean\n",
    "    if to_int:\n",
    "        image = tf.cast(image, tf.int32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary. Numpy version.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    img = np.copy(image)\n",
    "    img += np.array(means, dtype=img.dtype)\n",
    "    if to_int:\n",
    "        img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distort_color(image, scope=None):\n",
    "    \"\"\"Distort the color of the image.\n",
    "\n",
    "    Each color distortion is non-commutative and thus ordering of the color ops\n",
    "    matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "    Rather then adding that level of complication, we select a distinct ordering\n",
    "    of color ops for each preprocessing thread.\n",
    "\n",
    "    Args:\n",
    "    image: Tensor containing single image.\n",
    "    thread_id: preprocessing thread ID.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    color-distorted image\n",
    "    \"\"\"\n",
    "    with tf.name_scope( scope, 'distort_color',[image]):\n",
    "        color_ordering = np.random.randint(2)\n",
    "\n",
    "        if color_ordering == 0:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        elif color_ordering == 1:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "\n",
    "        # The random_* ops do not necessarily clamp.\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff2e9038c0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# build a net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxtbox_300\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextboxNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'text_shape '\u001b[0m\u001b[0;34m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # build a net\n",
    "    text_net = txtbox_300.TextboxNet()\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "    \n",
    "    ## dataset provider\n",
    "    dataset = sythtextprovider.get_datasets('../data/sythtext/',file_pattern='50.tfrecord')\n",
    "    \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=2)\n",
    "    \n",
    "    [image, shape, glabels, gbboxes,height,width] = \\\n",
    "    data_provider.get(['image', 'shape',\n",
    "                     'object/label',\n",
    "                     'object/bbox','height','width'])\n",
    "    \n",
    "    bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n",
    "                    tf.shape(image),\n",
    "                    bounding_boxes=tf.expand_dims(gbboxes, 0),\n",
    "                    min_object_covered=0.5,\n",
    "                    aspect_ratio_range=(0.8,1.2),\n",
    "                    area_range=(0.1,1.0),\n",
    "                    max_attempts=200,\n",
    "                    use_image_if_no_bounding_boxes=True)\n",
    "    \n",
    "    distort_bbox = distort_bbox[0, 0]\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    bboxes = tfe.bboxes_resize(distort_bbox, gbboxes)\n",
    "    labels, bboxes, num = tfe.bboxes_filter_overlap(glabels, bboxes, 0.5)\n",
    "    dst_image ,bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(cropped_image, bboxes,300,300)\n",
    "    #dst_image, bboxes = tf_image.random_flip_left_right(cropped_image, bboxes)\n",
    "    dst_image = tf.cast(dst_image,tf.float32)\n",
    "    dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n",
    "    dst_image = tf_image.distort_color_2(dst_image)\n",
    "    #dst_image = dst_image * 255.\n",
    "    dst_image = tf_image_whitened(dst_image, [123., 117., 104.])\n",
    "    dst_image.set_shape([300, 300, 3])\n",
    "    #dst_image = distort_color(dst_image)\n",
    "    # why take distort_color\n",
    "    image_p = tf.expand_dims(dst_image, 0)\n",
    "    image_p = tf.cast(image_p, tf.float32)\n",
    "    bboxes_p = tf.expand_dims(bboxes, 0)\n",
    "    bboxes_p = tf.maximum(bboxes_p, 0.0)\n",
    "    image_with_box = tf.image.draw_bounding_boxes(image_p, bboxes_p)\n",
    "    \n",
    "    \n",
    "    bboxes_test = tf.minimum(bboxes, 1.0)\n",
    "    # groud truth\n",
    "    glocalisations, gscores = \\\n",
    "    text_net.bboxes_encode( bboxes, text_anchors,num,match_threshold = 0.5)\n",
    "    \n",
    "    bbox_image = tf.image.draw_bounding_boxes(tf.expand_dims(dst_image,0), tf.expand_dims(bboxes,0))\n",
    "    \n",
    "    # batch\n",
    "    batch_shape = [1] + [6] * 2\n",
    "    r = tf.train.batch(\n",
    "        tf_utils.reshape_list([dst_image, glocalisations, gscores]),\n",
    "        batch_size=4,\n",
    "        num_threads=1,\n",
    "        capacity=2)\n",
    "    b_image, b_glocalisations, b_gscores= \\\n",
    "           tf_utils.reshape_list(r, batch_shape)\n",
    "     \n",
    "    inputs = b_image  \n",
    "\n",
    "    ## net predict\n",
    "    localisations, logits, end_points = \\\n",
    "    text_net.net(b_image, is_training=True)\n",
    "    \n",
    "    ## loss\n",
    "    \n",
    "    total_loss = text_net.losses(logits, localisations,\n",
    "           b_glocalisations, b_gscores,\n",
    "           match_threshold=0.5,\n",
    "           negative_ratio=3,\n",
    "           alpha=1,\n",
    "           label_smoothing=0)\n",
    "    \n",
    "    l_cross_pos = []\n",
    "    l_cross_neg = []\n",
    "    l_loc = []\n",
    "    n_poses = []\n",
    "    pmasks = []\n",
    "    for i in range(len(logits)):\n",
    "        dtype = logits[i].dtype\n",
    "        with tf.name_scope('block_%i' % i):\n",
    "\n",
    "            # Determine weights Tensor.\n",
    "            pmask = b_gscores[i] > 0.5\n",
    "            ipmask = tf.cast(pmask, tf.int32)\n",
    "            n_pos = tf.reduce_sum(ipmask)\n",
    "            fpmask = tf.cast(pmask, tf.float32)\n",
    "            nmask = b_gscores[i] < 0.5\n",
    "            inmask = tf.cast(nmask, tf.int32)\n",
    "            fnmask = tf.cast(nmask, tf.float32)\n",
    "            num = tf.ones_like(b_gscores[i])\n",
    "            n = tf.reduce_sum(num) + 1e-5\n",
    "            n_poses.append(n_pos)\n",
    "            pmasks.append(pmask)\n",
    "            # Add cross-entropy loss.\n",
    "        with tf.name_scope('cross_entropy_pos'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=ipmask)\n",
    "            #loss = tf.square(fpmask*(logits[i][:,:,:,:,:,1] - fpmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_pos.append(loss)\n",
    "        with tf.name_scope('cross_entropy_neg'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=inmask)\n",
    "            #loss = tf.square(fnmask*(logits[i][:,:,:,:,:,0] - fnmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_neg.append(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "    ## Training \n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                image_, bbox_, num_, bbox_img,image_bbox_p,l_cross_pos_,l_cross_neg_,n_poses_,loss_ = \\\n",
    "                sess.run([dst_image,bboxes,num,bbox_image,image_with_box,l_cross_pos,l_cross_neg,n_poses,total_loss])\n",
    "                box_test = sess.run([bboxes_test])\n",
    "                #print name\n",
    "                #print height,width\n",
    "                #height, width = shape[0],shape[1]\n",
    "                print loss_\n",
    "                print n_poses_\n",
    "                print l_cross_pos_, l_cross_neg_\n",
    "                print num_\n",
    "                #print len(glabels)\n",
    "                print bbox_.shape\n",
    "                print image_.shape\n",
    "\n",
    "                image_ = np.uint8(image_)*255\n",
    "                visualize_bbox(image_, bbox_)\n",
    "                \n",
    "                #bbox_img = sess.run(bbox_image)\n",
    "                #anchors_ = sess.run([text_anchors])\n",
    "                print text_anchors[0][0].shape\n",
    "                \n",
    "                glocalisations_, gscores_ = \\\n",
    "                sess.run([glocalisations, gscores])\n",
    "                print glocalisations_[0].shape\n",
    "            \n",
    "                \n",
    "                b_image_, b_glocalisations_, b_gscores_ = sess.run([b_image, b_glocalisations, b_gscores])\n",
    "                r_ = sess.run(r)\n",
    "                localisations_, logits_, end_points_ =\\\n",
    "                sess.run([localisations, logits, end_points])\n",
    "  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_bbox_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-269cec7a7031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_bbox_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bbox_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bbox_p\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bbox_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_bbox_p' is not defined"
     ]
    }
   ],
   "source": [
    "image_s = (image_bbox_p - np.min(image_bbox_p))*255/(np.max(image_bbox_p) - np.min(image_bbox_p))\n",
    "image_s = image_s.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13405cc10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136101750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#skio.imshow(np.uint8(image_bbox_p[0,:,:,:])*255)\n",
    "skio.imshow(image_s[0,:,:,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36474955,  0.61708784,  0.58035988,  0.9549982 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    pmask = gscores_[i] > 0.5\n",
    "    print np.sum(pmask)\n",
    "bbox_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 12, 12, 12, 12]), array([12, 13,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12,\n",
      "       13, 13, 13, 14, 14, 14,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n",
      "        9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 15, 15,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 14,\n",
      "       14,  8,  8,  9, 13]), array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 38, 2, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print np.where(gscores_[i] >0.1)\n",
    "gscores_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.05319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.00275449878623 6.05501136631\n"
     ]
    }
   ],
   "source": [
    "pos_loss = 0\n",
    "neg_loss = 0\n",
    "for i in range(6):\n",
    "    p_mask = np.int32(np.greater(b_gscores_[i] , 0.5))\n",
    "    print np.sum(p_mask)\n",
    "    n_mask = np.int32(np.less(b_gscores_[i] , 0.5))\n",
    "    pos_loss += np.mean(pow((p_mask * (logits_[i][:,:,:,:,:,1] - p_mask)),2))\n",
    "    neg_loss += np.mean(pow((n_mask * (logits_[i][:,:,:,:,:,0] - n_mask)),2))\n",
    "print pos_loss,neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: ../data/sythtext/*.tfrecord\n",
      "[197.63361]\n",
      "[197.36108]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # initalize the net\n",
    "    net = txtbox_300.TextboxNet()\n",
    "    out_shape = net.params.img_shape\n",
    "    anchors = net.anchors(out_shape)\n",
    "\n",
    "    # Create global_step.\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # create batch dataset\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        b_image, b_glocalisations, b_gscores = \\\n",
    "        load_batch.get_batch('../data/sythtext/',\n",
    "                             1,\n",
    "                             2,\n",
    "                             out_shape,\n",
    "                             net,\n",
    "                             anchors,\n",
    "                             1,\n",
    "                             is_training = True)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        arg_scope = net.arg_scope(weight_decay=0.0005)\n",
    "\n",
    "        with slim.arg_scope(arg_scope):\n",
    "            localisations, logits, end_points = \\\n",
    "                    net.net(b_image, is_training=True)\n",
    "\n",
    "        # Add loss function.\n",
    "        total_loss = net.losses(logits, localisations,\n",
    "                           b_glocalisations, b_gscores,\n",
    "                           match_threshold=0.5,\n",
    "                           negative_ratio=3,\n",
    "                           alpha=10,\n",
    "                           label_smoothing=0.0)\n",
    "\n",
    "    # Gather summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    for end_point in end_points:\n",
    "        x = end_points[end_point]\n",
    "        summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "        summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                        tf.nn.zero_fraction(x)))\n",
    "\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for loss in tf.get_collection('EXTRA_LOSSES'):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for variable in slim.get_model_variables():\n",
    "        summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        #learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "          #                                               FLAGS.num_samples,\n",
    "        #                                              global_step)\n",
    "        # Configure the optimization procedure \n",
    "        #optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        ## Training \n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(total_loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "\n",
    "    # =================================================================== #\n",
    "    # Kicks off the training.\n",
    "    # =================================================================== #\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "    config = tf.ConfigProto(log_device_placement=False,\n",
    "                            allow_soft_placement = True)\n",
    "    saver = tf.train.Saver(tf.global_variables(),\n",
    "                           max_to_keep=5,\n",
    "                           keep_checkpoint_every_n_hours=1.0,\n",
    "                           write_version=2,\n",
    "                           pad_step_number=False)\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                loss = sess.run([total_loss])\n",
    "                if i % 1 ==0:\n",
    "                    print loss\n",
    "                #current_step = tf.train.global_step(sess, global_step)\n",
    "                '''\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print i\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
