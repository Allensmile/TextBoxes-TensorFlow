{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1. Transform data to record format\n",
    "## First dataset from http://www.robots.ox.ac.uk/~vgg/data/scenetext/\n",
    "## This method failed, because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import gzip\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'../processing/')\n",
    "from datasets import sythtextprovider\n",
    "import tensorflow as tf\n",
    "import skimage.io as skio\n",
    "#tf.InteractiveSession()\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "slim = tf.contrib.slim\n",
    "tf.__version__\n",
    "#from image_processing2 import *\n",
    "from processing import ssd_vgg_preprocessing\n",
    "import tf_extended as tfe\n",
    "from processing import tf_image\n",
    "from nets import txtbox_300\n",
    "import tf_utils\n",
    "from nets import custom_layers\n",
    "import load_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordBB',\n",
       " 'txt',\n",
       " '__header__',\n",
       " '__globals__',\n",
       " '__version__',\n",
       " 'imnames',\n",
       " 'charBB']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntext = sio.loadmat('../data/sythtext/gt.mat')\n",
    "syntext.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/ballet_106_107.jpg\n"
     ]
    }
   ],
   "source": [
    "wordBB = syntext['wordBB']\n",
    "imnames = syntext['imnames']\n",
    "txt = syntext['txt']\n",
    "print imnames[0,10][0]\n",
    "index = 10\n",
    "img = cv2.imread(imnames[0,index][0])\n",
    "bbox = wordBB[0,index]\n",
    "text = txt[0,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 418.89144897  466.56863403  464.68301392  417.00582886]\n",
      " [ 230.17230225  233.0020752   264.77197266  261.94219971]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 417.00582886,  230.17230225], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print bbox[:,:,0]\n",
    "np.min(bbox[:,:,0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bboxes):\n",
    "    \"\"\"\n",
    "    Input: image (height, width, channels)\n",
    "           bboxes (numof bboxes, 4) in order(ymin, xmin, ymax, xmax)\n",
    "                  range(0,1) \n",
    "    \"\"\"\n",
    "    numofbox = bboxes.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "    def norm(x):\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        else:\n",
    "            if x > 1:\n",
    "                x = 1\n",
    "        return x\n",
    "    xmin = [int(i * width) for i in bboxes[:,1]]\n",
    "    ymin = [int(i * height) for i in bboxes[:,0]]\n",
    "    ymax = [int(i * height) for i in bboxes[:,2]]\n",
    "    xmax = [int(i * width) for i in bboxes[:,3]]\n",
    "\n",
    "    for i in range(numofbox):\n",
    "        image = cv2.rectangle(image,(xmin[i],ymin[i]),\n",
    "                             (xmax[i],ymax[i]),(0,255,255))\n",
    "    print [ymin,xmin,ymax,xmax]\n",
    "    skio.imshow(image)\n",
    "    skio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_R_MEAN = 123.\n",
    "_G_MEAN = 117.\n",
    "_B_MEAN = 104.\n",
    "\n",
    "# Some training pre-processing parameters.\n",
    "BBOX_CROP_OVERLAP = 0.4        # Minimum overlap to keep a bbox after cropping.\n",
    "CROP_RATIO_RANGE = (0.8, 1.2)  # Distortion ratio during cropping.\n",
    "EVAL_SIZE = (300, 300)\n",
    "def tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n",
    "    \"\"\"Subtracts the given means from each image channel.\n",
    "\n",
    "    Returns:\n",
    "        the centered image.\n",
    "    \"\"\"\n",
    "    if image.get_shape().ndims != 3:\n",
    "        raise ValueError('Input must be of size [height, width, C>0]')\n",
    "    num_channels = image.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image - mean\n",
    "    return image\n",
    "\n",
    "\n",
    "def tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image + mean\n",
    "    if to_int:\n",
    "        image = tf.cast(image, tf.int32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary. Numpy version.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    img = np.copy(image)\n",
    "    img += np.array(means, dtype=img.dtype)\n",
    "    if to_int:\n",
    "        img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distort_color(image, scope=None):\n",
    "    \"\"\"Distort the color of the image.\n",
    "\n",
    "    Each color distortion is non-commutative and thus ordering of the color ops\n",
    "    matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "    Rather then adding that level of complication, we select a distinct ordering\n",
    "    of color ops for each preprocessing thread.\n",
    "\n",
    "    Args:\n",
    "    image: Tensor containing single image.\n",
    "    thread_id: preprocessing thread ID.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    color-distorted image\n",
    "    \"\"\"\n",
    "    with tf.name_scope( scope, 'distort_color',[image]):\n",
    "        color_ordering = np.random.randint(2)\n",
    "\n",
    "        if color_ordering == 0:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        elif color_ordering == 1:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "\n",
    "        # The random_* ops do not necessarily clamp.\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_shape (300, 300)\n",
      "file_path: ../data/sythtext/50.tfrecord\n",
      "141.534\n",
      "[57, 70, 22, 0, 0, 0]\n",
      "[6.8432574, 7.1979599, 6.8959994, 6.9498706, 6.9506531, 6.9470286] [7.0370808, 6.7279663, 6.9857044, 6.9169068, 6.9132018, 6.9162755]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(2, 4)\n",
      "(300, 300, 3)\n",
      "[[105, 180], [104, 34], [206, 299], [206, 117]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFhhJREFUeJzt3XGsZOV53/HvD0KgGCpDcDYbWAlo15Ggqpd0RSzZiuxa\nMYR/wGqFlkoIKbTrSNg1khsFiNRQRYvc2saq1Bh3LZBpS0xWAQSyqBEgKsuSDSwUA7trwtqAYLWw\nxTiCCoGz9z79Y86FKTvn3pl7Z87M3Pv9SEdz5p0zc557xD68533P+76pKiRJxzpu2gFI0qwyQUpS\nCxOkJLUwQUpSCxOkJLUwQUpSi4klyCQXJ3kuycEk103qPJI0KZnEc5BJjgf+FvgD4BXgceCKqto/\n9pNJ0oRMqgZ5IXCwqn5eVb8C7gQundC5JGkifm1Cv3sm8HLf+1eA3+s/IMlOYCfACSf82j/7jdM/\nPKFQJE3Dq6+9/npVfWTp/UWf/lD94o2Fob//xNPvPlBVF08kuCFNKkGuqKp2A7sBNv/WR+qPrvwX\n0wpF0gTc9LX/+lL/+9ffWODRB84a+vsnbP7ZGWMPakSTusU+BGzpe39WUyZpwyoWanHobSVJtiR5\nJMn+JPuSfKkpvzHJoSRPNdslfd+5vuk4fi7JRSudY1I1yMeBrUnOoZcYdwD/akLnkjQHClhkrJ3C\nR4EvV9WTSU4FnkjyYPPZN6rqa/0HJzmPXi46H/ht4KEkH62q1vv+iSTIqjqa5AvAA8DxwG1VtW8S\n55I0PxZZuWY4rKo6DBxu9t9KcoBe/0ebS4E7q+pd4IUkB+l1KP+o7QsTew6yqu6vqo9W1T+qql2T\nOo+k+VAUCzX8BpyRZG/ftrPtt5OcDVwAPNoUfTHJ00luS3JaUzao83i5hOpIGkndWaSG3oDXq2p7\n37Z70G8mOQW4C7i2qt4EbgHOBbbRq2F+fbXxTq0XW9LGUsDCeNsgSXICveR4R1XdDVBVr/V9/m3g\ne83bkTuPrUFK6syINchlJQlwK3Cgqm7uK9/cd9jngGeb/fuAHUlObDqQtwKPLXcOa5CSOlGw1LY4\nLp8ArgSeSfJUU3YDcEWSbc0pXwQ+D1BV+5LsAfbT6wG/ZrkebDBBSurQ+Pqwoap+CGTAR/cv851d\nwNCdxiZISZ0oauxtkJNmgpTUjYKF+cqPJkhJ3eiNpJkvJkhJHQkLA5sMZ5cJUlInClj0FluSBrMG\nKUkD9EbSmCAlaaDFMkFK0jGsQUpSiyIszNn0DyZISZ3xFluSBvAWW5JahYXyFluSjtEbamiClKRj\nVIVf1fHTDmMkJkhJnVm0DVKSjtXrpPEWW5IGsJNGkgayk0aSlrHgg+KSdCyHGkrSMhZtg5SkY9mL\nLUktitgGKUlt7MWWpAGq8DlISRosG2uoYZIXgbeABeBoVW1Pcjrw18DZwIvA5VX1y7WFKWneFfNX\ngxxHtJ+uqm1Vtb15fx3wcFVtBR5u3ksSCxw39DYLJhHFpcDtzf7twGUTOIekOVOExRp+mwVrTZAF\nPJTkiSQ7m7JNVXW42X8V2DToi0l2JtmbZO/bb7+zxjAkzYN5q0GutZPmk1V1KMlvAg8m+Wn/h1VV\nSWrQF6tqN7AbYPNvfWTgMZLWj2KDjaSpqkPN65Ek9wAXAq8l2VxVh5NsBo6MIU5Jcy9zt2jXqtN5\nkg8lOXVpH/gs8CxwH3BVc9hVwL1rDVLS/FuqQQ67zYK11CA3AfckWfqdv6qq7yd5HNiT5GrgJeDy\ntYcpaT2YtxrkqhNkVf0c+NiA8l8An1lLUJLWn6rMTM1wWI6kkdSZeXtQ3AQpqRO9JRc2yC22JI2i\nCH+/OF/rYs9XfVfSXBvng+JJtiR5JMn+JPuSfKkpPz3Jg0meb15P6/vO9UkOJnkuyUUrncMEKakT\nExhqeBT4clWdB3wcuCbJebTMB9F8tgM4H7gY+GaSZau0JkhJnVnkuKG3lVTV4ap6stl/CzgAnEn7\nfBCXAndW1btV9QJwkN7glla2QUrqRG/C3JE6ac5Isrfv/e5miPIxkpwNXAA8Svt8EGcCP+772itN\nWSsTpKTOjDhLz+t90yi2SnIKcBdwbVW92QxeAZafD2IYJkhJnei1QY63VS/JCfSS4x1VdXdT3DYf\nxCFgS9/Xz2rKWtkGKakzC82EFcNsK0mvqngrcKCqbu77qG0+iPuAHUlOTHIOsBV4bLlzWIOU1Ine\nZBVjfVD8E8CVwDNJnmrKbgC+woD5IKpqX5I9wH56PeDXVNXCcicwQUrqyHhvsavqh9Ba1Rw4H0RV\n7QJ2DXsOE6SkzjjUUJIGWMVjPlNngpTUGac7k6QBloYazhMTpKTO2AYpSQNM4DGfiTNBSuqMbZCS\nNMjw05jNDBOkpE645IIkLcMapCQNYCeNJC3DBClJA/iguCQtw04aSRqgCo4u+hykJA3kLbYkDWAb\npCQto0yQkjSYnTSSNEDV/LVBrtillOS2JEeSPNtXdnqSB5M837ye1vfZ9UkOJnkuyUWTClzS/KnK\n0NssGKbP/TvAxR8ouw54uKq2Ag8370lyHrADOL/5zjeTHD+2aCXNsV4nzbDbLFgxQVbVD4A3PlB8\nKXB7s387cFlf+Z1V9W5VvQAcBC4cU6yS5ty81SBX2wa5qaoON/uvApua/TOBH/cd90pTdowkO4Gd\nAP/w1FNWGYakeTGPk1Ws+bH2qip6f/uo39tdVduravvJJ5+01jAkzbrqddQMu82C1dYgX0uyuaoO\nJ9kMHGnKDwFb+o47qymTpLl7zGe1Ncj7gKua/auAe/vKdyQ5Mck5wFbgsbWFKGk9KNZhG2SS7wKf\nAs5I8grw58BXgD1JrgZeAi4HqKp9SfYA+4GjwDVVtTCh2LWMm776rWmHsO7c8Cd/PO0Q5tzs9E4P\na8UEWVVXtHz0mZbjdwG71hKUxsN/0OPj/3DGY1baFoflSBpJnZmVW+dhmSAldaLXO22ClKSB1l0b\npCSNi22QktTCW2xJGqCYnecbh2WClNSZObvDNkFK6oi92JK0jDmrQpogJXVmcXG+apDztYq3pLk1\n7skqWpaDuTHJoSRPNdslfZ+NvByMCVJSNwqoDL+t7DscuxwMwDeqaluz3Q+rXw7GBCmpM+OcMLdl\nOZg2q1oOxgQpqTs1wtabYnFv37ZzyLN8McnTzS340oqrZwIv9x3TuhxMPztpJHVk5AfFX6+q7SOe\n5BbgL+il2L8Avg780Yi/8R5rkJK6M1oNcvSfr3qtqhaqahH4Nu/fRq9qORgTpKRu1OSXXGjWyFry\nOWCph3tVy8F4iy2pO2N8ULxlOZhPJdnWnOlF4POw+uVgTJCSOjS+B8VbloO5dZnjR14OxgQpqTsO\nNZSkFiZISRpgaSTNHDFBSuqMSy5IUhsTpCS18BZbkgaLNUhJGmANQwinxQQpqSNDz/M4M0yQkrpj\nDVKSWpggJamFCVKSBpjDkTQrzgfZxcphkjaG1PDbLBhmwtzvMOGVwyRtEBOeUXzcVkyQXawcJmlj\nWI81yDZrWjksyc6l1crefvudNYQhaW6Md13siVttgrwFOBfYBhymt3LYSKpqd1Vtr6rtJ5980irD\nkDQ3Rrm9nuca5LhXDpO0QWyEBDnulcMkbQzz1ga54nOQXawcJmmDmJHEN6wVE2QXK4dJ2iDWW4KU\npHGYpVvnYZkgJXVnRh7fGZYJUlJ3rEFK0mDeYktSGxOkJA1gJ40kLcMEKUktTJCSNNi83WKvZboz\nSVrXrEFK6s6c1SBNkJK6YS+2JC3DBClJLUyQknSs4C22JLUzQUrSAAVZnHYQo/E5SEndGeOiXc2S\n00eSPNtXdnqSB5M837ye1vfZ9UkOJnkuyUXDhGuClNSZMS/a9R3g4g+UXQc8XFVbgYeb9yQ5D9gB\nnN9855tJjl/pBBsyQd701W9NO4SZ4vVQZ8ZYg6yqHwBvfKD4UuD2Zv924LK+8jur6t2qegE4yPvL\nVbfakAlS0hSMkhx7CfKMJHv7tp1DnGVTVR1u9l8FNjX7ZwIv9x33SlO2LDtpJHVmxMd8Xq+q7as9\nV1VVsrYHi6xBSurOGG+xW7yWZDNA83qkKT8EbOk77qymbFkmSEmdGXMnzSD3AVc1+1cB9/aV70hy\nYpJzgK3AYyv9mLfYkrozxgfFk3wX+BS9tspXgD8HvgLsSXI18BJwOUBV7UuyB9gPHAWuqaqFlc5h\ngpTUjbXdOh/7c1VXtHz0mZbjdwG7RjmHCVJSJ9Js88QEKak7jsWWpMGczUeS2pggJamFCVKSBpjD\nNWlWfFA8yZYkjyTZn2Rfki815WOdVkjSBjD5kTRjNcxImqPAl6vqPODjwDXN1EFjnVZI0vrXwUia\nsVoxQVbV4ap6stl/CzhAbxaMsU4rJGkDWIc1yPckORu4AHiUNU4rlGTn0jRGb7/9zohhS5pH664G\nuSTJKcBdwLVV9Wb/Z1U1cs6vqt1Vtb2qtp988kmjfFXSPBp9PsipGypBJjmBXnK8o6ruborHOq2Q\npA1gvSXIJAFuBQ5U1c19H411WiFJ69vSutjzdIs9zHOQnwCuBJ5J8lRTdgNjnlZI0gYwI4lvWCsm\nyKr6Ie2TcIxtWiGN3yiLcblwlyauIIvzlSEdSbNO3fAnfzz0sTd99VsjHS+t1qzcOg/LBCmpOyZI\nSRrMGqQktTFBStIAM/T4zrBMkJK6Y4KUpGMtPSg+T0yQkrpT85UhTZCSOmMNUpIGmaFJKIZlgpTU\nmSxOO4LRmCAldccapCQNZhukJA1S2IstSW2sQUpSGxOkJB3LkTSS1KbKNkhJamMNUpLamCAlaTBr\nkJI0SAGuaihJLeYrP5ogJXXHdbElqcW42yCTvAi8BSwAR6tqe5LTgb8GzgZeBC6vql+u5vePG0+Y\nkrSCGnEb3qeraltVbW/eXwc8XFVbgYeb96tigpTUid5Imhp6W4NLgdub/duBy1b7QyZISd1ZHGEb\nTgEPJXkiyc6mbFNVHW72XwU2rTZc2yAldWbEmuEZSfb2vd9dVbs/cMwnq+pQkt8EHkzy0/4Pq6qS\n1bd8miAldWP0tsXX+9oVB/9k1aHm9UiSe4ALgdeSbK6qw0k2A0dWGbG32JK6Uu9PWDHMtoIkH0py\n6tI+8FngWeA+4KrmsKuAe1cbsTVISZ0Z82M+m4B7kkAvl/1VVX0/yePAniRXAy8Bl6/2BCsmyCRb\ngP/WBFP02gH+c5IbgX8D/J/m0Buq6v7mO9cDV9N7NunfVtUDqw1Q0joyxunOqurnwMcGlP8C+Mw4\nzjFMDfIo8OWqerKpzj6R5MHms29U1df6D05yHrADOB/4bXo9TB+tqoVxBCxpTtX8Lfu6YhtkVR2u\nqieb/beAA8CZy3zlUuDOqnq3ql4ADtJrOJW00Y2xDbILI3XSJDkbuAB4tCn6YpKnk9yW5LSm7Ezg\n5b6vvcKAhJpkZ5K9Sfa+/fY7IwcuaQ5NZiTNxAydIJOcAtwFXFtVbwK3AOcC24DDwNdHOXFV7a6q\n7VW1/eSTTxrlq5LmVEcjacZmqF7sJCfQS453VNXdAFX1Wt/n3wa+17w9BGzp+/pZTZmkjW5GEt+w\nVqxBpteHfitwoKpu7ivf3HfY5+g9fwS9Z5B2JDkxyTnAVuCx8YUsaS4VkxhqOFHD1CA/AVwJPJPk\nqabsBuCKJNvo/dkvAp8HqKp9SfYA++n1gF9jD7akMDu3zsNaMUFW1Q/pTcTxQfcv851dwK41xCVp\nPVpvCVKSxsYEKUkDLLVBzhETpKTOrLs2SEkaGxOkJA0yO0MIh2WClNSNwgQpSW2yYIKUpMGsQUrS\nAAUsmiAlaQA7aSSpnQlSklqYICVpANsgJalNQc3XYGwTpKTueIstSQN4iy1Jy7AGKUktTJCSNIgP\nikvSYAUs2ostSYNZg5SkFiZISRqkfMxHkgYqKEfSSFILa5CS1MI2SEkaoMrHfCSplTVISRqsrEFK\n0iAONZSkwQpYWJh2FCMxQUrqRAE1Z4/5HLfSAUlOSvJYkp8k2ZfkPzTlpyd5MMnzzetpfd+5PsnB\nJM8luWiSf4CkOVHNkgvDbkNIcnGTZw4muW7cIa+YIIF3gX9eVR8DtgEXJ/k4cB3wcFVtBR5u3pPk\nPGAHcD5wMfDNJMePO3BJ86cWa+htJU1e+UvgD4HzgCua/DM2K95iV1UB/7d5e0KzFXAp8Kmm/Hbg\nfwF/2pTfWVXvAi8kOQhcCPxonIGv1U1f/da0Q5A2nvEONbwQOFhVPwdIcie9/LN/XCdIDdGr1GTq\nJ4B/DPxlVf1pkr+rqg83nwf4ZVV9OMl/AX5cVf+j+exW4H9W1d984Dd3Ajubt78D/AJ4fUx/16Sc\ngTGOgzGOx6zH+DtVderSmyTfpxfzsE4C3ul7v7uqdvf93r8ELq6qf928vxL4var6wtrCft9QnTRV\ntQBsS/Jh4J4k/+QDn1eSkVpfmz+0/4/dW1XbR/mNrhnjeBjjeMx6jEn29r+vqounFctqDdMG+Z6q\n+jvgEXpti68l2QzQvB5pDjsEbOn72llNmSSN08RzzTC92B9pao4k+QfAHwA/Be4DrmoOuwq4t9m/\nD9iR5MQk5wBbgcfGGbQkAY8DW5Ock+TX6XUO3zfOEwxzi70ZuL1phzwO2FNV30vyI2BPkquBl4DL\nAapqX5I99BpKjwLXNLfoK9m98iFTZ4zjYYzjMesxTjS+qjqa5AvAA8DxwG1VtW+c5xiqk0aSNqKR\n2iAlaSMxQUpSi6knyEkPFVqtJC8meSbJU0uPKyw3vLKjmG5LciTJs31lMzXksyXGG5Mcaq7lU0ku\nmXKMW5I8kmR/M3z2S035zFzLZWKcmWu5IYYhV9XUNnoNqz8DzgV+HfgJcN40Y+qL7UXgjA+U/Sfg\numb/OuA/dhzT7wO/Czy7Ukz0hl79BDgROKe5zsdPKcYbgX834NhpxbgZ+N1m/1Tgb5tYZuZaLhPj\nzFxLIMApzf4JwKPAx2fpOq51m3YN8r2hQlX1K2BpqNCsupTesEqa18u6PHlV/QB4Y8iY3hvyWVUv\nAEtDPqcRY5tpxXi4qp5s9t8CDgBnMkPXcpkY20wjxqqqtmHIM3Ed12raCfJM4OW+96+w/H8EXSrg\noSRPNMMiATZV1eFm/1Vg03RC+/+0xTRr1/aLSZ5ubsGXbrmmHmOSs4EL6NV+ZvJafiBGmKFrmeT4\nJE/RGyjyYFXN7HVcjWknyFn2yaraRm+mkGuS/H7/h9W7Z5ipZ6RmMabGLfSaUbYBh4GvTzecniSn\nAHcB11bVm/2fzcq1HBDjTF3Lqlpo/p2cBVw4aBgyM3AdV2vaCXJmhyVW1aHm9QhwD71bgbbhldM0\n80M+q+q15h/SIvBt3r+tmlqMSU6gl3juqKq7m+KZupaDYpzFa9nEtS6HIU87QU58qNBqJPlQklOX\n9oHPAs/SPrxymmZ+yOfSP5bG5+hdS5hSjEkC3AocqKqb+z6amWvZFuMsXctshGHI0+4lAi6h10P3\nM+DPph1PE9O59HrbfgLsW4oL+A16kwM/DzwEnN5xXN+ld1v19/Tab65eLibgz5rr+hzwh1OM8b8D\nzwBP0/tHsnnKMX6S3m3f08BTzXbJLF3LZWKcmWsJ/FPgfzexPAv8+6Z8Zq7jWjeHGkpSi2nfYkvS\nzDJBSlILE6QktTBBSlILE6QktTBBSlILE6Qktfh/Nv6GiUCMJBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x139907e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n",
      "140.888\n",
      "[413, 302, 22, 0, 0, 0]\n",
      "[6.8434663, 7.1866517, 6.896225, 6.9498882, 6.9506521, 6.9470401] [7.0369296, 6.7394581, 6.9855237, 6.9168997, 6.9132037, 6.9162636]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(5, 4)\n",
      "(300, 300, 3)\n",
      "[[194, 12, 218, 175, 209], [212, 33, -24, 110, 1], [240, 35, 245, 210, 227], [342, 92, 80, 260, 69]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpplQElq1v8BHfTdyTg/00kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12341d5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # build a net\n",
    "    text_net = txtbox_300.TextboxNet()\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "    \n",
    "    ## dataset provider\n",
    "    dataset = sythtextprovider.get_datasets('../data/sythtext/',file_pattern='50.tfrecord')\n",
    "    \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=2)\n",
    "    \n",
    "    [image, shape, glabels, gbboxes,height,width] = \\\n",
    "    data_provider.get(['image', 'shape',\n",
    "                     'object/label',\n",
    "                     'object/bbox','height','width'])\n",
    "    \n",
    "    bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n",
    "                    tf.shape(image),\n",
    "                    bounding_boxes=tf.expand_dims(gbboxes, 0),\n",
    "                    min_object_covered=0.5,\n",
    "                    aspect_ratio_range=(0.9,1.1),\n",
    "                    area_range=(0.1,1.0),\n",
    "                    max_attempts=200,\n",
    "                    use_image_if_no_bounding_boxes=True)\n",
    "    \n",
    "    distort_bbox = distort_bbox[0, 0]\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    bboxes = tfe.bboxes_resize(distort_bbox, gbboxes)\n",
    "    labels, bboxes, num = tfe.bboxes_filter_overlap(glabels, bboxes, 0.5)\n",
    "    dst_image ,bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(cropped_image, bboxes,300,300)\n",
    "    #dst_image, bboxes = tf_image.random_flip_left_right(cropped_image, bboxes)\n",
    "    dst_image = tf.cast(dst_image,tf.float32)\n",
    "    dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n",
    "    dst_image = tf_image.distort_color_2(dst_image)\n",
    "    #dst_image = dst_image * 255.\n",
    "    dst_image = tf_image_whitened(dst_image, [123., 117., 104.])\n",
    "    dst_image.set_shape([300, 300, 3])\n",
    "    #dst_image = distort_color(dst_image)\n",
    "    # why take distort_color\n",
    "    image_p = tf.expand_dims(dst_image, 0)\n",
    "    image_p = tf.cast(image_p, tf.float32)\n",
    "    bboxes_p = tf.expand_dims(bboxes, 0)\n",
    "    bboxes_p = tf.maximum(bboxes_p, 0.0)\n",
    "    image_with_box = tf.image.draw_bounding_boxes(image_p, bboxes_p)\n",
    "    \n",
    "    \n",
    "    bboxes_test = tf.minimum(bboxes, 1.0)\n",
    "    # groud truth\n",
    "    glocalisations, gscores = \\\n",
    "    text_net.bboxes_encode( bboxes, text_anchors,num,match_threshold = 0.5)\n",
    "    \n",
    "    bbox_image = tf.image.draw_bounding_boxes(tf.expand_dims(dst_image,0), tf.expand_dims(bboxes,0))\n",
    "    \n",
    "    # batch\n",
    "    batch_shape = [1] + [6] * 2\n",
    "    r = tf.train.batch(\n",
    "        tf_utils.reshape_list([dst_image, glocalisations, gscores]),\n",
    "        batch_size=4,\n",
    "        num_threads=1,\n",
    "        capacity=2)\n",
    "    b_image, b_glocalisations, b_gscores= \\\n",
    "           tf_utils.reshape_list(r, batch_shape)\n",
    "     \n",
    "    inputs = b_image  \n",
    "\n",
    "    ## net predict\n",
    "    localisations, logits, end_points = \\\n",
    "    text_net.net(b_image, is_training=True)\n",
    "    \n",
    "    ## loss\n",
    "    \n",
    "    total_loss = text_net.losses(logits, localisations,\n",
    "           b_glocalisations, b_gscores,\n",
    "           match_threshold=0.5,\n",
    "           negative_ratio=3,\n",
    "           alpha=1,\n",
    "           label_smoothing=0)\n",
    "    \n",
    "    l_cross_pos = []\n",
    "    l_cross_neg = []\n",
    "    l_loc = []\n",
    "    n_poses = []\n",
    "    pmasks = []\n",
    "    for i in range(len(logits)):\n",
    "        dtype = logits[i].dtype\n",
    "        with tf.name_scope('block_%i' % i):\n",
    "\n",
    "            # Determine weights Tensor.\n",
    "            pmask = b_gscores[i] > 0.5\n",
    "            ipmask = tf.cast(pmask, tf.int32)\n",
    "            n_pos = tf.reduce_sum(ipmask)\n",
    "            fpmask = tf.cast(pmask, tf.float32)\n",
    "            nmask = b_gscores[i] < 0.5\n",
    "            inmask = tf.cast(nmask, tf.int32)\n",
    "            fnmask = tf.cast(nmask, tf.float32)\n",
    "            num = tf.ones_like(b_gscores[i])\n",
    "            n = tf.reduce_sum(num) + 1e-5\n",
    "            n_poses.append(n_pos)\n",
    "            pmasks.append(pmask)\n",
    "            # Add cross-entropy loss.\n",
    "        with tf.name_scope('cross_entropy_pos'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=ipmask)\n",
    "            #loss = tf.square(fpmask*(logits[i][:,:,:,:,:,1] - fpmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_pos.append(loss)\n",
    "        with tf.name_scope('cross_entropy_neg'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=inmask)\n",
    "            #loss = tf.square(fnmask*(logits[i][:,:,:,:,:,0] - fnmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_neg.append(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "    ## Training \n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                image_, bbox_, num_, bbox_img,image_bbox_p,l_cross_pos_,l_cross_neg_,n_poses_,loss_ = \\\n",
    "                sess.run([dst_image,bboxes,num,bbox_image,image_with_box,l_cross_pos,l_cross_neg,n_poses,total_loss])\n",
    "                box_test = sess.run([bboxes_test])\n",
    "                #print name\n",
    "                #print height,width\n",
    "                #height, width = shape[0],shape[1]\n",
    "                print loss_\n",
    "                print n_poses_\n",
    "                print l_cross_pos_, l_cross_neg_\n",
    "                print num_\n",
    "                #print len(glabels)\n",
    "                print bbox_.shape\n",
    "                print image_.shape\n",
    "\n",
    "                image_ = np.uint8(image_)*255\n",
    "                visualize_bbox(image_, bbox_)\n",
    "                \n",
    "                #bbox_img = sess.run(bbox_image)\n",
    "                #anchors_ = sess.run([text_anchors])\n",
    "                print text_anchors[0][0].shape\n",
    "                \n",
    "                glocalisations_, gscores_ = \\\n",
    "                sess.run([glocalisations, gscores])\n",
    "                print glocalisations_[0].shape\n",
    "            \n",
    "                \n",
    "                b_image_, b_glocalisations_, b_gscores_ = sess.run([b_image, b_glocalisations, b_gscores])\n",
    "                r_ = sess.run(r)\n",
    "                localisations_, logits_, end_points_ =\\\n",
    "                sess.run([localisations, logits, end_points])\n",
    "  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_s = (image_bbox_p - np.min(image_bbox_p))*255/(np.max(image_bbox_p) - np.min(image_bbox_p))\n",
    "image_s = image_s.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'mat' (pos 2) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0df70f5f10da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#skio.imshow(np.uint8(image_bbox_p[0,:,:,:])*255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#skio.imshow(image_s[0,:,:,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'mat' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "#skio.imshow(np.uint8(image_bbox_p[0,:,:,:])*255)\n",
    "skio.imshow(image_s[0,:,:,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36474955,  0.61708784,  0.58035988,  0.9549982 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    pmask = gscores_[i] > 0.5\n",
    "    print np.sum(pmask)\n",
    "bbox_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 12, 12, 12, 12]), array([12, 13,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12,\n",
      "       13, 13, 13, 14, 14, 14,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n",
      "        9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 15, 15,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 14,\n",
      "       14,  8,  8,  9, 13]), array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 38, 2, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print np.where(gscores_[i] >0.1)\n",
    "gscores_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.05319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.00275449878623 6.05501136631\n"
     ]
    }
   ],
   "source": [
    "pos_loss = 0\n",
    "neg_loss = 0\n",
    "for i in range(6):\n",
    "    p_mask = np.int32(np.greater(b_gscores_[i] , 0.5))\n",
    "    print np.sum(p_mask)\n",
    "    n_mask = np.int32(np.less(b_gscores_[i] , 0.5))\n",
    "    pos_loss += np.mean(pow((p_mask * (logits_[i][:,:,:,:,:,1] - p_mask)),2))\n",
    "    neg_loss += np.mean(pow((n_mask * (logits_[i][:,:,:,:,:,0] - n_mask)),2))\n",
    "print pos_loss,neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: ../data/sythtext/*.tfrecord\n",
      "[197.63361]\n",
      "[197.36108]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # initalize the net\n",
    "    net = txtbox_300.TextboxNet()\n",
    "    out_shape = net.params.img_shape\n",
    "    anchors = net.anchors(out_shape)\n",
    "\n",
    "    # Create global_step.\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # create batch dataset\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        b_image, b_glocalisations, b_gscores = \\\n",
    "        load_batch.get_batch('../data/sythtext/',\n",
    "                             1,\n",
    "                             2,\n",
    "                             out_shape,\n",
    "                             net,\n",
    "                             anchors,\n",
    "                             1,\n",
    "                             is_training = True)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        arg_scope = net.arg_scope(weight_decay=0.0005)\n",
    "\n",
    "        with slim.arg_scope(arg_scope):\n",
    "            localisations, logits, end_points = \\\n",
    "                    net.net(b_image, is_training=True)\n",
    "\n",
    "        # Add loss function.\n",
    "        total_loss = net.losses(logits, localisations,\n",
    "                           b_glocalisations, b_gscores,\n",
    "                           match_threshold=0.5,\n",
    "                           negative_ratio=3,\n",
    "                           alpha=10,\n",
    "                           label_smoothing=0.0)\n",
    "\n",
    "    # Gather summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    for end_point in end_points:\n",
    "        x = end_points[end_point]\n",
    "        summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "        summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                        tf.nn.zero_fraction(x)))\n",
    "\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for loss in tf.get_collection('EXTRA_LOSSES'):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for variable in slim.get_model_variables():\n",
    "        summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        #learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "          #                                               FLAGS.num_samples,\n",
    "        #                                              global_step)\n",
    "        # Configure the optimization procedure \n",
    "        #optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        ## Training \n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(total_loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "\n",
    "    # =================================================================== #\n",
    "    # Kicks off the training.\n",
    "    # =================================================================== #\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "    config = tf.ConfigProto(log_device_placement=False,\n",
    "                            allow_soft_placement = True)\n",
    "    saver = tf.train.Saver(tf.global_variables(),\n",
    "                           max_to_keep=5,\n",
    "                           keep_checkpoint_every_n_hours=1.0,\n",
    "                           write_version=2,\n",
    "                           pad_step_number=False)\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                loss = sess.run([total_loss])\n",
    "                if i % 1 ==0:\n",
    "                    print loss\n",
    "                #current_step = tf.train.global_step(sess, global_step)\n",
    "                '''\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print i\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
