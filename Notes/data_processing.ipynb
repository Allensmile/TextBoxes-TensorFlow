{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1. Transform data to record format\n",
    "## First dataset from http://www.robots.ox.ac.uk/~vgg/data/scenetext/\n",
    "## This method failed, because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import gzip\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'../processing/')\n",
    "from datasets import sythtextprovider\n",
    "import tensorflow as tf\n",
    "import skimage.io as skio\n",
    "#tf.InteractiveSession()\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "slim = tf.contrib.slim\n",
    "tf.__version__\n",
    "#from image_processing2 import *\n",
    "from processing import ssd_vgg_preprocessing\n",
    "import tf_extended as tfe\n",
    "from processing import tf_image\n",
    "from nets import txtbox_300\n",
    "import tf_utils\n",
    "from nets import custom_layers\n",
    "import load_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordBB',\n",
       " 'txt',\n",
       " '__header__',\n",
       " '__globals__',\n",
       " '__version__',\n",
       " 'imnames',\n",
       " 'charBB']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntext = sio.loadmat('../data/sythtext/gt.mat')\n",
    "syntext.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/ballet_106_107.jpg\n"
     ]
    }
   ],
   "source": [
    "wordBB = syntext['wordBB']\n",
    "imnames = syntext['imnames']\n",
    "txt = syntext['txt']\n",
    "print imnames[0,10][0]\n",
    "index = 10\n",
    "img = cv2.imread(imnames[0,index][0])\n",
    "bbox = wordBB[0,index]\n",
    "text = txt[0,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 418.89144897  466.56863403  464.68301392  417.00582886]\n",
      " [ 230.17230225  233.0020752   264.77197266  261.94219971]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 417.00582886,  230.17230225], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print bbox[:,:,0]\n",
    "np.min(bbox[:,:,0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bboxes):\n",
    "    \"\"\"\n",
    "    Input: image (height, width, channels)\n",
    "           bboxes (numof bboxes, 4) in order(ymin, xmin, ymax, xmax)\n",
    "                  range(0,1) \n",
    "    \"\"\"\n",
    "    numofbox = bboxes.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "    def norm(x):\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        else:\n",
    "            if x > 1:\n",
    "                x = 1\n",
    "        return x\n",
    "    xmin = [int(i * width) for i in bboxes[:,1]]\n",
    "    ymin = [int(i * height) for i in bboxes[:,0]]\n",
    "    ymax = [int(i * height) for i in bboxes[:,2]]\n",
    "    xmax = [int(i * width) for i in bboxes[:,3]]\n",
    "\n",
    "    for i in range(numofbox):\n",
    "        image = cv2.rectangle(image,(xmin[i],ymin[i]),\n",
    "                             (xmax[i],ymax[i]),(0,255,255))\n",
    "    print [ymin,xmin,ymax,xmax]\n",
    "    skio.imshow(image)\n",
    "    skio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_R_MEAN = 123.\n",
    "_G_MEAN = 117.\n",
    "_B_MEAN = 104.\n",
    "\n",
    "# Some training pre-processing parameters.\n",
    "BBOX_CROP_OVERLAP = 0.4        # Minimum overlap to keep a bbox after cropping.\n",
    "CROP_RATIO_RANGE = (0.8, 1.2)  # Distortion ratio during cropping.\n",
    "EVAL_SIZE = (300, 300)\n",
    "def tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n",
    "    \"\"\"Subtracts the given means from each image channel.\n",
    "\n",
    "    Returns:\n",
    "        the centered image.\n",
    "    \"\"\"\n",
    "    if image.get_shape().ndims != 3:\n",
    "        raise ValueError('Input must be of size [height, width, C>0]')\n",
    "    num_channels = image.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image - mean\n",
    "    return image\n",
    "\n",
    "\n",
    "def tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image + mean\n",
    "    if to_int:\n",
    "        image = tf.cast(image, tf.int32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary. Numpy version.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    img = np.copy(image)\n",
    "    img += np.array(means, dtype=img.dtype)\n",
    "    if to_int:\n",
    "        img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distort_color(image, scope=None):\n",
    "    \"\"\"Distort the color of the image.\n",
    "\n",
    "    Each color distortion is non-commutative and thus ordering of the color ops\n",
    "    matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "    Rather then adding that level of complication, we select a distinct ordering\n",
    "    of color ops for each preprocessing thread.\n",
    "\n",
    "    Args:\n",
    "    image: Tensor containing single image.\n",
    "    thread_id: preprocessing thread ID.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    color-distorted image\n",
    "    \"\"\"\n",
    "    with tf.name_scope( scope, 'distort_color',[image]):\n",
    "        color_ordering = np.random.randint(2)\n",
    "\n",
    "        if color_ordering == 0:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        elif color_ordering == 1:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "\n",
    "        # The random_* ops do not necessarily clamp.\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_shape (300, 300)\n",
      "file_path: ../data/sythtext/50.tfrecord\n",
      "5.5511\n",
      "[386, 109, 57, 8, 0, 0]\n",
      "[7.0099874, 7.3959846, 6.9240527, 6.9048567, 6.9911666, 6.9266577] [6.8617225, 6.5652828, 6.969233, 6.966815, 6.8733444, 6.9367032]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(3, 4)\n",
      "(300, 300, 3)\n",
      "[[142, 156, 157], [120, -2, 45], [198, 174, 177], [325, 31, 76]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npCS1MEFKUgsTpCS1+P/xMIklxb7wNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109d7e210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n",
      "5.47119\n",
      "[371, 222, 24, 11, 1, 0]\n",
      "[7.0104971, 7.3983665, 6.935791, 6.9124413, 6.9913721, 6.9265103] [6.8612385, 6.5642138, 6.9581738, 6.9591565, 6.8730478, 6.9369678]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(3, 4)\n",
      "(300, 300, 3)\n",
      "[[115, 122, 126], [152, 182, 237], [130, 138, 137], [211, 212, 261]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCVKS+jBBSlIf/x+Qf2jlkMiAsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13da70c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # build a net\n",
    "    text_net = txtbox_300.TextboxNet()\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "    \n",
    "    ## dataset provider\n",
    "    dataset = sythtextprovider.get_datasets('../data/sythtext/',file_pattern='50.tfrecord')\n",
    "    \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=2)\n",
    "    \n",
    "    [image, shape, glabels, gbboxes,height,width] = \\\n",
    "    data_provider.get(['image', 'shape',\n",
    "                     'object/label',\n",
    "                     'object/bbox','height','width'])\n",
    "    \n",
    "    bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n",
    "                    tf.shape(image),\n",
    "                    bounding_boxes=tf.expand_dims(gbboxes, 0),\n",
    "                    min_object_covered=0.5,\n",
    "                    aspect_ratio_range=(0.9,1.1),\n",
    "                    area_range=(0.1,1.0),\n",
    "                    max_attempts=200,\n",
    "                    use_image_if_no_bounding_boxes=True)\n",
    "    \n",
    "    distort_bbox = distort_bbox[0, 0]\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    bboxes = tfe.bboxes_resize(distort_bbox, gbboxes)\n",
    "    labels, bboxes, num = tfe.bboxes_filter_overlap(glabels, bboxes, 0.5)\n",
    "    dst_image ,bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(cropped_image, bboxes,300,300)\n",
    "    #dst_image, bboxes = tf_image.random_flip_left_right(cropped_image, bboxes)\n",
    "    dst_image = tf.cast(dst_image,tf.float32)\n",
    "    dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n",
    "    dst_image = tf_image.distort_color_2(dst_image)\n",
    "    #dst_image = dst_image * 255.\n",
    "    dst_image = tf_image_whitened(dst_image, [123., 117., 104.])\n",
    "    dst_image.set_shape([300, 300, 3])\n",
    "    #dst_image = distort_color(dst_image)\n",
    "    # why take distort_color\n",
    "    image_p = tf.expand_dims(dst_image, 0)\n",
    "    image_p = tf.cast(image_p, tf.float32)\n",
    "    bboxes_p = tf.expand_dims(bboxes, 0)\n",
    "    bboxes_p = tf.maximum(bboxes_p, 0.0)\n",
    "    image_with_box = tf.image.draw_bounding_boxes(image_p, bboxes_p)\n",
    "    \n",
    "    \n",
    "    bboxes_test = tf.minimum(bboxes, 1.0)\n",
    "    # groud truth\n",
    "    glocalisations, gscores = \\\n",
    "    text_net.bboxes_encode( bboxes, text_anchors,num,match_threshold = 0.5)\n",
    "    \n",
    "    bbox_image = tf.image.draw_bounding_boxes(tf.expand_dims(dst_image,0), tf.expand_dims(bboxes,0))\n",
    "    \n",
    "    # batch\n",
    "    batch_shape = [1] + [6] * 2\n",
    "    r = tf.train.batch(\n",
    "        tf_utils.reshape_list([dst_image, glocalisations, gscores]),\n",
    "        batch_size=4,\n",
    "        num_threads=1,\n",
    "        capacity=2)\n",
    "    b_image, b_glocalisations, b_gscores= \\\n",
    "           tf_utils.reshape_list(r, batch_shape)\n",
    "     \n",
    "    inputs = b_image  \n",
    "\n",
    "    ## net predict\n",
    "    localisations, logits, end_points = \\\n",
    "    text_net.net(b_image, is_training=True)\n",
    "    \n",
    "    ## loss\n",
    "    \n",
    "    total_loss = text_net.losses(logits, localisations,\n",
    "           b_glocalisations, b_gscores,\n",
    "           match_threshold=0.5,\n",
    "           negative_ratio=3,\n",
    "           alpha=1,\n",
    "           label_smoothing=0)\n",
    "    \n",
    "    l_cross_pos = []\n",
    "    l_cross_neg = []\n",
    "    l_loc = []\n",
    "    n_poses = []\n",
    "    pmasks = []\n",
    "    for i in range(len(logits)):\n",
    "        dtype = logits[i].dtype\n",
    "        with tf.name_scope('block_%i' % i):\n",
    "\n",
    "            # Determine weights Tensor.\n",
    "            pmask = b_gscores[i] > 0.5\n",
    "            ipmask = tf.cast(pmask, tf.int32)\n",
    "            n_pos = tf.reduce_sum(ipmask)\n",
    "            fpmask = tf.cast(pmask, tf.float32)\n",
    "            nmask = b_gscores[i] < 0.5\n",
    "            inmask = tf.cast(nmask, tf.int32)\n",
    "            fnmask = tf.cast(nmask, tf.float32)\n",
    "            num = tf.ones_like(b_gscores[i])\n",
    "            n = tf.reduce_sum(num) + 1e-5\n",
    "            n_poses.append(n_pos)\n",
    "            pmasks.append(pmask)\n",
    "            # Add cross-entropy loss.\n",
    "        with tf.name_scope('cross_entropy_pos'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=ipmask)\n",
    "            #loss = tf.square(fpmask*(logits[i][:,:,:,:,:,1] - fpmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_pos.append(loss)\n",
    "        with tf.name_scope('cross_entropy_neg'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=inmask)\n",
    "            #loss = tf.square(fnmask*(logits[i][:,:,:,:,:,0] - fnmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_neg.append(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "    ## Training \n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                image_, bbox_, num_, bbox_img,image_bbox_p,l_cross_pos_,l_cross_neg_,n_poses_,loss_ = \\\n",
    "                sess.run([dst_image,bboxes,num,bbox_image,image_with_box,l_cross_pos,l_cross_neg,n_poses,total_loss])\n",
    "                box_test = sess.run([bboxes_test])\n",
    "                #print name\n",
    "                #print height,width\n",
    "                #height, width = shape[0],shape[1]\n",
    "                print loss_\n",
    "                print n_poses_\n",
    "                print l_cross_pos_, l_cross_neg_\n",
    "                print num_\n",
    "                #print len(glabels)\n",
    "                print bbox_.shape\n",
    "                print image_.shape\n",
    "\n",
    "                image_ = np.uint8(image_)*255\n",
    "                visualize_bbox(image_, bbox_)\n",
    "                \n",
    "                #bbox_img = sess.run(bbox_image)\n",
    "                #anchors_ = sess.run([text_anchors])\n",
    "                print text_anchors[0][0].shape\n",
    "                \n",
    "                glocalisations_, gscores_ = \\\n",
    "                sess.run([glocalisations, gscores])\n",
    "                print glocalisations_[0].shape\n",
    "            \n",
    "                \n",
    "                b_image_, b_glocalisations_, b_gscores_ = sess.run([b_image, b_glocalisations, b_gscores])\n",
    "                r_ = sess.run(r)\n",
    "                localisations_, logits_, end_points_ =\\\n",
    "                sess.run([localisations, logits, end_points])\n",
    "  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_s = (image_bbox_p - np.min(image_bbox_p))*255/(np.max(image_bbox_p) - np.min(image_bbox_p))\n",
    "image_s = image_s.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13405cc10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136101750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#skio.imshow(np.uint8(image_bbox_p[0,:,:,:])*255)\n",
    "skio.imshow(image_s[0,:,:,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36474955,  0.61708784,  0.58035988,  0.9549982 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    pmask = gscores_[i] > 0.5\n",
    "    print np.sum(pmask)\n",
    "bbox_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 12, 12, 12, 12]), array([12, 13,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12,\n",
      "       13, 13, 13, 14, 14, 14,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n",
      "        9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 15, 15,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 14,\n",
      "       14,  8,  8,  9, 13]), array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 38, 2, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print np.where(gscores_[i] >0.1)\n",
    "gscores_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.05319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.00275449878623 6.05501136631\n"
     ]
    }
   ],
   "source": [
    "pos_loss = 0\n",
    "neg_loss = 0\n",
    "for i in range(6):\n",
    "    p_mask = np.int32(np.greater(b_gscores_[i] , 0.5))\n",
    "    print np.sum(p_mask)\n",
    "    n_mask = np.int32(np.less(b_gscores_[i] , 0.5))\n",
    "    pos_loss += np.mean(pow((p_mask * (logits_[i][:,:,:,:,:,1] - p_mask)),2))\n",
    "    neg_loss += np.mean(pow((n_mask * (logits_[i][:,:,:,:,:,0] - n_mask)),2))\n",
    "print pos_loss,neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: ../data/sythtext/*.tfrecord\n",
      "[197.63361]\n",
      "[197.36108]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # initalize the net\n",
    "    net = txtbox_300.TextboxNet()\n",
    "    out_shape = net.params.img_shape\n",
    "    anchors = net.anchors(out_shape)\n",
    "\n",
    "    # Create global_step.\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # create batch dataset\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        b_image, b_glocalisations, b_gscores = \\\n",
    "        load_batch.get_batch('../data/sythtext/',\n",
    "                             1,\n",
    "                             2,\n",
    "                             out_shape,\n",
    "                             net,\n",
    "                             anchors,\n",
    "                             1,\n",
    "                             is_training = True)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        arg_scope = net.arg_scope(weight_decay=0.0005)\n",
    "\n",
    "        with slim.arg_scope(arg_scope):\n",
    "            localisations, logits, end_points = \\\n",
    "                    net.net(b_image, is_training=True)\n",
    "\n",
    "        # Add loss function.\n",
    "        total_loss = net.losses(logits, localisations,\n",
    "                           b_glocalisations, b_gscores,\n",
    "                           match_threshold=0.5,\n",
    "                           negative_ratio=3,\n",
    "                           alpha=10,\n",
    "                           label_smoothing=0.0)\n",
    "\n",
    "    # Gather summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    for end_point in end_points:\n",
    "        x = end_points[end_point]\n",
    "        summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "        summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                        tf.nn.zero_fraction(x)))\n",
    "\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for loss in tf.get_collection('EXTRA_LOSSES'):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for variable in slim.get_model_variables():\n",
    "        summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        #learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "          #                                               FLAGS.num_samples,\n",
    "        #                                              global_step)\n",
    "        # Configure the optimization procedure \n",
    "        #optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        ## Training \n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(total_loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "\n",
    "    # =================================================================== #\n",
    "    # Kicks off the training.\n",
    "    # =================================================================== #\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "    config = tf.ConfigProto(log_device_placement=False,\n",
    "                            allow_soft_placement = True)\n",
    "    saver = tf.train.Saver(tf.global_variables(),\n",
    "                           max_to_keep=5,\n",
    "                           keep_checkpoint_every_n_hours=1.0,\n",
    "                           write_version=2,\n",
    "                           pad_step_number=False)\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                loss = sess.run([total_loss])\n",
    "                if i % 1 ==0:\n",
    "                    print loss\n",
    "                #current_step = tf.train.global_step(sess, global_step)\n",
    "                '''\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print i\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
