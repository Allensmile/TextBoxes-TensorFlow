{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1. Transform data to record format\n",
    "## First dataset from http://www.robots.ox.ac.uk/~vgg/data/scenetext/\n",
    "## This method failed, because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import gzip\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'../processing/')\n",
    "from datasets import sythtextprovider\n",
    "import tensorflow as tf\n",
    "import skimage.io as skio\n",
    "#tf.InteractiveSession()\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "slim = tf.contrib.slim\n",
    "tf.__version__\n",
    "#from image_processing2 import *\n",
    "from processing import ssd_vgg_preprocessing\n",
    "import tf_extended as tfe\n",
    "from processing import tf_image\n",
    "from nets import txtbox_300\n",
    "import tf_utils\n",
    "from nets import custom_layers\n",
    "import load_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordBB',\n",
       " 'txt',\n",
       " '__header__',\n",
       " '__globals__',\n",
       " '__version__',\n",
       " 'imnames',\n",
       " 'charBB']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntext = sio.loadmat('../data/sythtext/gt.mat')\n",
    "syntext.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/ballet_106_107.jpg\n"
     ]
    }
   ],
   "source": [
    "wordBB = syntext['wordBB']\n",
    "imnames = syntext['imnames']\n",
    "txt = syntext['txt']\n",
    "print imnames[0,10][0]\n",
    "index = 10\n",
    "img = cv2.imread(imnames[0,index][0])\n",
    "bbox = wordBB[0,index]\n",
    "text = txt[0,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 418.89144897  466.56863403  464.68301392  417.00582886]\n",
      " [ 230.17230225  233.0020752   264.77197266  261.94219971]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 417.00582886,  230.17230225], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print bbox[:,:,0]\n",
    "np.min(bbox[:,:,0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Wrapper for inserting float features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Wrapper for inserting bytes features into Example proto.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bboxes):\n",
    "    \"\"\"\n",
    "    Input: image (height, width, channels)\n",
    "           bboxes (numof bboxes, 4) in order(ymin, xmin, ymax, xmax)\n",
    "                  range(0,1) \n",
    "    \"\"\"\n",
    "    numofbox = bboxes.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "    def norm(x):\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        else:\n",
    "            if x > 1:\n",
    "                x = 1\n",
    "        return x\n",
    "    xmin = [int(i * width) for i in bboxes[:,1]]\n",
    "    ymin = [int(i * height) for i in bboxes[:,0]]\n",
    "    ymax = [int(i * height) for i in bboxes[:,2]]\n",
    "    xmax = [int(i * width) for i in bboxes[:,3]]\n",
    "\n",
    "    for i in range(numofbox):\n",
    "        image = cv2.rectangle(image,(xmin[i],ymin[i]),\n",
    "                             (xmax[i],ymax[i]),(0,255,255))\n",
    "    print [ymin,xmin,ymax,xmax]\n",
    "    skio.imshow(image)\n",
    "    skio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_R_MEAN = 123.\n",
    "_G_MEAN = 117.\n",
    "_B_MEAN = 104.\n",
    "\n",
    "# Some training pre-processing parameters.\n",
    "BBOX_CROP_OVERLAP = 0.4        # Minimum overlap to keep a bbox after cropping.\n",
    "CROP_RATIO_RANGE = (0.8, 1.2)  # Distortion ratio during cropping.\n",
    "EVAL_SIZE = (300, 300)\n",
    "def tf_image_whitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN]):\n",
    "    \"\"\"Subtracts the given means from each image channel.\n",
    "\n",
    "    Returns:\n",
    "        the centered image.\n",
    "    \"\"\"\n",
    "    if image.get_shape().ndims != 3:\n",
    "        raise ValueError('Input must be of size [height, width, C>0]')\n",
    "    num_channels = image.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image - mean\n",
    "    return image\n",
    "\n",
    "\n",
    "def tf_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    mean = tf.constant(means, dtype=image.dtype)\n",
    "    image = image + mean\n",
    "    if to_int:\n",
    "        image = tf.cast(image, tf.int32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def np_image_unwhitened(image, means=[_R_MEAN, _G_MEAN, _B_MEAN], to_int=True):\n",
    "    \"\"\"Re-convert to original image distribution, and convert to int if\n",
    "    necessary. Numpy version.\n",
    "\n",
    "    Returns:\n",
    "      Centered image.\n",
    "    \"\"\"\n",
    "    img = np.copy(image)\n",
    "    img += np.array(means, dtype=img.dtype)\n",
    "    if to_int:\n",
    "        img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distort_color(image, scope=None):\n",
    "    \"\"\"Distort the color of the image.\n",
    "\n",
    "    Each color distortion is non-commutative and thus ordering of the color ops\n",
    "    matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "    Rather then adding that level of complication, we select a distinct ordering\n",
    "    of color ops for each preprocessing thread.\n",
    "\n",
    "    Args:\n",
    "    image: Tensor containing single image.\n",
    "    thread_id: preprocessing thread ID.\n",
    "    scope: Optional scope for op_scope.\n",
    "    Returns:\n",
    "    color-distorted image\n",
    "    \"\"\"\n",
    "    with tf.name_scope( scope, 'distort_color',[image]):\n",
    "        color_ordering = np.random.randint(2)\n",
    "\n",
    "        if color_ordering == 0:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        elif color_ordering == 1:\n",
    "          image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "          image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "          image = tf.image.random_hue(image, max_delta=0.2)\n",
    "\n",
    "        # The random_* ops do not necessarily clamp.\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_shape (300, 300)\n",
      "file_path: ../data/sythtext/50.tfrecord\n",
      "8.2974\n",
      "[434, 73, 5, 0, 0, 0]\n",
      "[6.8255644, 6.6221952, 7.0348377, 6.8857584, 6.9393964, 6.9466281] [7.0473404, 7.2781687, 6.8384166, 6.9791069, 6.9238386, 6.9164209]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(5, 4)\n",
      "(300, 300, 3)\n",
      "[[241, 290, 139, -4, -31], [83, -114, 50, 2, 267], [270, 339, 175, 46, 1], [203, 171, 306, 116, 329]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKh7KKQRUeosAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1246b2f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n",
      "11.1283\n",
      "[292, 36, 4, 8, 6, 0]\n",
      "[6.8248024, 6.6202788, 7.0350637, 6.8868237, 6.9394541, 6.9466381] [7.0481224, 7.2800155, 6.8381662, 6.9780369, 6.9237761, 6.9164109]\n",
      "[[[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ 1.  1.  1.  1.  1.  1.]\n",
      "    [ 1.  1.  1.  1.  1.  1.]]]]]\n",
      "(2, 4)\n",
      "(300, 300, 3)\n",
      "[[60, 95], [56, 13], [122, 164], [148, 114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/python/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/skimage/io/_plugins/matplotlib_plugin.py:74: UserWarning: Low image dynamic range; displaying image with stretched contrast.\n",
      "  warn(\"Low image dynamic range; displaying image with \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEYCAYAAAA+mm/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFjFJREFUeJzt3X+sHeV95/H3B9eFJbAq1KnrGkuGXaeSiRrTtWikRFWy\nUYOLKploJWQqUUu16kRySZDYSIZIG1aVUXaTEPWPhvRGoLgrGmItIKwIFYEVCUVKAEMd8I+QOMEI\nWxe7DqkgQiHxvZ/948wlZ+2Zc+fcO2fOmXs/L2nkOc+ZOfP1CH955nnmeR7ZJiIiLnTRuAOIiJhU\nSZARERWSICMiKiRBRkRUSIKMiKiQBBkRUWFkCVLSFkkvSzouafeorhMRMSoaxXuQklYAPwL+DDgJ\nPAfcYvto4xeLiBiRUdUgrweO2/6p7V8BDwFbR3StiIiR+K0R/e5a4LW+zyeBP+k/QNJOYCfAypW/\n9V9+98rfGVEoETEOr58+e9b2e+c+3/DR9/hnb8zUPv/5F995wvaWkQRX06gS5LxsTwFTAGt+/73+\n61v/27hCiYgRuOdL//hq/+ezb8zwzBNX1T5/5ZqfrGo8qCGN6hH7FLCu7/NVRVlELFtmxrO1t/lI\nWifpO5KOSjoi6TNF+d2STkk6VGw39p1zZ9Fx/LKkG+a7xqhqkM8BGyRdTS8xbgP+ckTXiogOMDBL\no53C54A7bL8g6XLgeUlPFt99xfaX+g+WtJFeLroW+APgKUnvs1353D+SBGn7nKS/BZ4AVgAP2D4y\nimtFRHfMMn/NsC7b08B0sf+WpGP0+j+qbAUesv0O8Iqk4/Q6lL9XdcLI3oO0/bjt99n+T7b3jOo6\nEdENxsy4/gasknSwb9tZ9duS1gPXAc8URbdJelHSA5KuKMrKOo8HJdSMpImI9szi2htw1vbmvm2q\n7DclXQY8DNxu+03gPuAaYBO9GuaXFxrv2HqxI2J5MTDTbBskklbSS44P2n4EwPbpvu+/Dny7+Dh0\n53FqkBHRmiFrkANJEnA/cMz2vX3la/oO+wRwuNjfD2yTdHHRgbwBeHbQNVKDjIhWGObaFpvyIeBW\n4CVJh4qyu4BbJG0qLnkC+CSA7SOS9gFH6fWA7xrUgw1JkBHRoub6sMH2dwGVfPX4gHP2ALU7jZMg\nI6IVxo23QY5aEmREtMMw0638mAQZEe3ojaTpliTIiGiJmCltMpxcSZAR0QoDs3nEjogolxpkRESJ\n3kiaJMiIiFKzToKMiLhAapARERWMmOnY9A9JkBHRmjxiR0SUyCN2REQlMeM8YkdEXKA31DAJMiLi\nArb4lVeMO4yhJEFGRGtm0wYZEXGhXidNHrEjIkqkkyYiolQ6aSIiBpjJi+IRERfKUMOIiAFm0wYZ\nEXGh9GJHRFQwShtkRESV9GJHRJSwyXuQERHltLyGGko6AbwFzADnbG+WdCXwLWA9cAK42fbPFxdm\nRHSd6V4NsoloP2p7k+3NxefdwAHbG4ADxeeICGa4qPY2CUYRxVZgb7G/F7hpBNeIiI4xYtb1t0mw\n2ARp4ClJz0vaWZSttj1d7L8OrC47UdJOSQclHXz77V8uMoyI6IKu1SAX20nzYdunJP0e8KSkH/Z/\naduSXHai7SlgCmDN77+39JiIWDrMMhtJY/tU8ecZSY8C1wOnJa2xPS1pDXCmgTgjovPUuUW7FpzO\nJb1H0uVz+8DHgcPAfmB7cdh24LHFBhkR3TdXg6y7TYLF1CBXA49Kmvudf7b9L5KeA/ZJ2gG8Cty8\n+DAjYinoWg1ywQnS9k+BD5SU/wz42GKCioilx9bE1AzrykiaiGhN114UT4KMiFb0llxYJo/YAfd8\n8WvjDqFT7vrsp8YdQoyREb+ezbrYy0r+0deT/5kENDthrqR1wD/R6zA2MGX77wfNByHpTmAHvfkj\nPm37iUHX6FaDQER01giGGp4D7rC9EfggsEvSRirmgyi+2wZcC2wBvippYJU2CTIiWjPLRbW3+die\ntv1Csf8WcAxYS/V8EFuBh2y/Y/sV4Di9wS2V8ogdEa3oTZg7VCfNKkkH+z5PFUOULyBpPXAd8AzV\n80GsBb7fd9rJoqxSEmREtGbIWXrO9k2jWEnSZcDDwO223ywGrwCD54OoIwkyIlrRa4NstlVP0kp6\nyfFB248UxVXzQZwC1vWdflVRViltkBHRmpliwoo623zUqyreDxyzfW/fV1XzQewHtkm6WNLVwAbg\n2UHXSA0yIlrRm6yi0RfFPwTcCrwk6VBRdhfwBUrmg7B9RNI+4Ci9HvBdtmcGXSAJMiJa0uwjtu3v\nQmVVs3Q+CNt7gD11r5EEGRGtyVDDiIgSC3jNZ+ySICOiNZnuLCKixNxQwy5JgoyI1qQNMiKixAhe\n8xm5JMiIaE3aICMiytSfxmxiJEFGRCuy5EJExACpQUZElEgnTUTEAEmQEREl8qJ4RMQA6aSJiChh\nw7nZvAcZEVEqj9gRESXSBhkRMYCTICMiyqWTZsTu+eLXxh3C/2fS4hnGXZ/91LhDiGXEXoJtkJIe\nAP4COGP7/UXZlcC3gPXACeBm2z8vvrsT2AHMAJ+2/USTAU/SP+p7vvi1iYpnGF1O7NFdXXvErtPn\n/g1gy3llu4EDtjcAB4rPSNoIbAOuLc75qqQVjUUbER3W66Spu02CeROk7aeBN84r3grsLfb3Ajf1\nlT9k+x3brwDHgesbijUiOs5W7W0SLLQNcrXt6WL/dWB1sb8W+H7fcSeLsgtI2gnsBPiPl1+2wDAi\noiu6OFnFol9rt216f/dhz5uyvdn25ksvvWSxYUTEpHOvo6buNgkWWoM8LWmN7WlJa4AzRfkpYF3f\ncVcVZRERnXvNZ6E1yP3A9mJ/O/BYX/k2SRdLuhrYADy7uBAjYikwS7ANUtI3gY8AqySdBD4PfAHY\nJ2kH8CpwM4DtI5L2AUeBc8Au2zMjij0iOmVyeqfrmjdB2r6l4quPVRy/B9izmKAiYmmalLbFujo3\nkiYiumtSHp3rSoKMiFb0eqeTICMiSi25NsiIiKakDTIiokIesSMiSpjJeb+xriTIiGhNx56wkyAj\noiXpxY6IGKBjVcgkyIhozexst2qQ3VrFOyI6q+nJKiQ9IOmMpMN9ZXdLOiXpULHd2PfdnZKOS3pZ\n0g11Yk6CjIh2GLDqb/P7BhcuBwPwFdubiu1xWPhyMHnEXqQuL37V5dijm5p8Udz205LW1zz83eVg\ngFckzS0H871BJyVBLkJXVzSEbq/IGB02XIJcJelg3+cp21M1zrtN0l8BB4E7ihVXay8H0y8JMiJa\nMvSL4mdtbx7yIvcBf0cvFf8d8GXgr4f8jXelDTIi2uMhtoX8vH3a9oztWeDr/GZV1QUtB5MEGRHt\n8OiXXCjWyJrzCWCuh3tBy8HkETsi2tNgJ03FcjAfkbSpuNIJ4JOw8OVgkiAjokXNvShesRzM/QOO\nH3o5mCTIiGhPhhpGRFRIgoyIKDE3kqZDkiAjojVZciEiokoSZEREhTxiR0SUU2qQERElFjGEcFyS\nICOiJbXneZwYSZAR0Z7UICMiKiRBRkRUSIKMiCjRwZE0884H2cbKYRGxPMj1t0lQZ8LcbzDilcMi\nYpkY8YziTZs3Qdp+Gnij5u+9u3KY7VeAuZXDIiKWZA2yym2SXiwewa8oytYCr/UdU7lymKSdkg5K\nOvj2279cRBgR0RnNros9cgtNkPcB1wCbgGl6K4cNxfaU7c22N1966SULDCMiOmOYx+su1yCbXjks\nIpaJ5ZAgm145LCKWh661Qc77HmQbK4dFxDIxIYmvrnkTZBsrh0XEMrHUEmRERBMm6dG5riTIiGjP\nhLy+U1cSZES0JzXIiIhyecSOiKiSBDla93zxa+MOISIWIp00o3fXZz817hAiYqGSICMiKiRBRkSU\n69oj9mKmO4uIWNJSg4yI9nSsBpkEGRHtSC92RMQASZARERWSICMiLiTyiB0RUS0JMiKihEGz4w5i\nOHkPMiLa0+CiXcWS02ckHe4ru1LSk5J+XPx5Rd93d0o6LullSTfUCTcJMiJa0/CiXd8AtpxXths4\nYHsDcKD4jKSNwDbg2uKcr0paMd8FkiAjoj0N1iBtPw28cV7xVmBvsb8XuKmv/CHb79h+BTjOb5ar\nrpQEGRHtGCY59hLkKkkH+7adNa6y2vZ0sf86sLrYXwu81nfcyaJsoHTSRERrhnzN56ztzQu9lm1L\ni3uxKDXIiGhPg4/YFU5LWgNQ/HmmKD8FrOs77qqibKAkyIhoTcOdNGX2A9uL/e3AY33l2yRdLOlq\nYAPw7Hw/lkfsiGhPgy+KS/om8BF6bZUngc8DXwD2SdoBvArcDGD7iKR9wFHgHLDL9sx810iCjIh2\nLO7R+cKfs2+p+OpjFcfvAfYMc40kyIhohYqtS5IgI6I9GYsdEVEus/lERFRJgoyIqJAEGRFRooNr\n0sz7orikdZK+I+mopCOSPlOUNzqtUEQsA6MfSdOoOiNpzgF32N4IfBDYVUwd1Oi0QhGx9LUwkqZR\n8yZI29O2Xyj23wKO0ZsFo9FphSJiGViCNch3SVoPXAc8wyKnFZK0c24ao7ff/uWQYUdEFy25GuQc\nSZcBDwO3236z/zvbQ+d821O2N9vefOmllwxzakR00fDzQY5drQQpaSW95Pig7UeK4kanFYqIZWCp\nJUhJAu4Hjtm+t++rRqcVioilbW5d7C49Ytd5D/JDwK3AS5IOFWV30fC0QhGxDExI4qtr3gRp+7tU\nT8LR2LRCEbHEGTTbrQyZkTQR0ZpJeXSuKwkyItqTBBkRUS41yIiIKkmQERElJuj1nbqSICOiPUmQ\nEREXmntRvEuSICOiPe5WhkyCjIjWpAYZEVFmgiahqCsJMiJao9lxRzCcJMiIaE9qkBER5dIGGRFR\nxqQXOyKiSmqQERFVkiAjIi6UkTQREVXstEFGRFRJDTIiokoSZEREudQgIyLKGMiqhhERFbqVH5Mg\nI6I9WRc7IqJC022Qkk4AbwEzwDnbmyVdCXwLWA+cAG62/fOF/P5FzYQZETEPD7nV91Hbm2xvLj7v\nBg7Y3gAcKD4vSBJkRLSiN5LGtbdF2ArsLfb3Ajct9IeSICOiPbNDbPUYeErS85J2FmWrbU8X+68D\nqxcabtogI6I1Q9YMV0k62Pd5yvbUecd82PYpSb8HPCnph/1f2ra08JbPJMiIaMfwbYtn+9oVy3/S\nPlX8eUbSo8D1wGlJa2xPS1oDnFlgxHnEjoi2+DcTVtTZ5iHpPZIun9sHPg4cBvYD24vDtgOPLTTi\n1CAjojUNv+azGnhUEvRy2T/b/hdJzwH7JO0AXgVuXugF5k2QktYB/1QEY3rtAH8v6W7gb4B/Kw69\ny/bjxTl3AjvovZv0adtPLDTAiFhCGpzuzPZPgQ+UlP8M+FgT16hTgzwH3GH7haI6+7ykJ4vvvmL7\nS/0HS9oIbAOuBf6AXg/T+2zPNBFwRHSUu7fs67xtkLanbb9Q7L8FHAPWDjhlK/CQ7XdsvwIcp9dw\nGhHLXYNtkG0YqpNG0nrgOuCZoug2SS9KekDSFUXZWuC1vtNOUpJQJe2UdFDSwbff/uXQgUdEB41m\nJM3I1E6Qki4DHgZut/0mcB9wDbAJmAa+PMyFbU/Z3mx786WXXjLMqRHRUS2NpGlMrV5sSSvpJccH\nbT8CYPt03/dfB75dfDwFrOs7/aqiLCKWuwlJfHXNW4NUrw/9fuCY7Xv7ytf0HfYJeu8fQe8dpG2S\nLpZ0NbABeLa5kCOik8wohhqOVJ0a5IeAW4GXJB0qyu4CbpG0id5f+wTwSQDbRyTtA47S6wHflR7s\niBCT8+hc17wJ0vZ36U3Ecb7HB5yzB9iziLgiYilaagkyIqIxSZARESXm2iA7JAkyIlqz5NogIyIa\nkwQZEVFmcoYQ1pUEGRHtMEmQERFVNJMEGRFRLjXIiIgSBmaTICMiSqSTJiKiWhJkRESFJMiIiBJp\ng4yIqGJwtwZjJ0FGRHvyiB0RUSKP2BERA6QGGRFRIQkyIqJMXhSPiChnYDa92BER5VKDjIiokAQZ\nEVHGec0nIqKUwRlJExFRITXIiIgKaYOMiChh5zWfiIhKqUFGRJRzapAREWUy1DAiopyBmZlxRzGU\nJMiIaIUBd+w1n4vmO0DSJZKelfQDSUck/c+i/EpJT0r6cfHnFX3n3CnpuKSXJd0wyr9ARHSEiyUX\n6m41SNpS5JnjknY3HfK8CRJ4B/ivtj8AbAK2SPogsBs4YHsDcKD4jKSNwDbgWmAL8FVJK5oOPCK6\nx7Ouvc2nyCv/APw5sBG4pcg/jZk3QbrnF8XHlcVmYCuwtyjfC9xU7G8FHrL9ju1XgOPA9U0GHREd\n1WwN8nrguO2f2v4V8BC9/NOYWm2QRaZ+HvjPwD/YfkbSatvTxSGvA6uL/bXA9/tOP1mUnf+bO4Gd\nxcdf3POlf/wZcHb4v0KrVpEYm5AYmzHpMf5h/4e3+PkTT/n/rhri/EskHez7PGV7qu/zWuC1vs8n\ngT8ZPsxqtRKk7Rlgk6TfAR6V9P7zvrekoVpfi7/ou39ZSQdtbx7mN9qWGJuRGJsx6TGel9ywvWVc\nsSxUnTbId9n+d+A79NoWT0taA1D8eaY47BSwru+0q4qyiIgmjTzX1OnFfm9Rc0TSfwD+DPghsB/Y\nXhy2HXis2N8PbJN0saSrgQ3As00GHREBPAdskHS1pN+m1zm8v8kL1HnEXgPsLdohLwL22f62pO8B\n+yTtAF4FbgawfUTSPuAocA7YVTyiz2dq/kPGLjE2IzE2Y9JjHGl8ts9J+lvgCWAF8IDtI01eQ+7Y\n0J+IiLYM1QYZEbGcJEFGRFQYe4Ic9VChhZJ0QtJLkg7Nva4waHhlSzE9IOmMpMN9ZRM15LMixrsl\nnSru5SFJN445xnWSviPpaDF89jNF+cTcywExTsy9XBbDkG2PbaPXsPoT4Brgt4EfABvHGVNfbCeA\nVeeV/W9gd7G/G/hfLcf0p8AfA4fni4ne0KsfABcDVxf3ecWYYrwb+O8lx44rxjXAHxf7lwM/KmKZ\nmHs5IMaJuZeAgMuK/ZXAM8AHJ+k+LnYbdw1y5EOFGlY1vLIVtp8G3qgZ01iGfFbEWGVcMU7bfqHY\nfws4Rm9UxsTcywExVhlHjPYSH4Y87gRZNlRo0H8EbTLwlKTni2GRAFXDK8dp0JDPSbq3t0l6sXgE\nn3vkGnuMktYD19Gr/UzkvTwvRpigeylphaRD9AaKPGl7Yu/jQow7QU6yD9veRG+mkF2S/rT/S/ee\nGSbqHalJjKlwH71mlE3ANPDl8YbTI+ky4GHgdttv9n83KfeyJMaJupe2Z4p/J1cB15cNQ2YC7uNC\njTtBTuywRNunij/PAI/SexSoGl45ThM/5NP26eIf0izwdX7zWDW2GCWtpJd4HrT9SFE8UfeyLMZJ\nvJdFXEtyGPK4E+TIhwothKT3SLp8bh/4OHCY6uGV4zTxQz7n/rEUPkHvXsKYYpQk4H7gmO17+76a\nmHtZFeMk3Usth2HI4+4lAm6k10P3E+Bz446niOkaer1tPwCOzMUF/C69yYF/DDwFXNlyXN+k91j1\na3rtNzsGxQR8rrivLwN/PsYY/w/wEvAivX8ka8Yc44fpPfa9CBwqthsn6V4OiHFi7iXwR8C/FrEc\nBv5HUT4x93GxW4YaRkRUGPcjdkTExEqCjIiokAQZEVEhCTIiokISZEREhSTIiIgKSZARERX+H/RB\ndZaBCb5WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125ce3890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 38, 2, 1)\n",
      "(38, 38, 2, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # build a net\n",
    "    text_net = txtbox_300.TextboxNet()\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "    \n",
    "    ## dataset provider\n",
    "    dataset = sythtextprovider.get_datasets('../data/sythtext/',file_pattern='50.tfrecord')\n",
    "    \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=2)\n",
    "    \n",
    "    [image, shape, glabels, gbboxes,height,width] = \\\n",
    "    data_provider.get(['image', 'shape',\n",
    "                     'object/label',\n",
    "                     'object/bbox','height','width'])\n",
    "    \n",
    "    bbox_begin, bbox_size, distort_bbox = tf.image.sample_distorted_bounding_box(\n",
    "                    tf.shape(image),\n",
    "                    bounding_boxes=tf.expand_dims(gbboxes, 0),\n",
    "                    min_object_covered=0.5,\n",
    "                    aspect_ratio_range=(0.9,1.1),\n",
    "                    area_range=(0.1,1.0),\n",
    "                    max_attempts=200,\n",
    "                    use_image_if_no_bounding_boxes=True)\n",
    "    \n",
    "    distort_bbox = distort_bbox[0, 0]\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    bboxes = tfe.bboxes_resize(distort_bbox, gbboxes)\n",
    "    labels, bboxes, num = tfe.bboxes_filter_overlap(glabels, bboxes, 0.5)\n",
    "    dst_image ,bboxes = tf_image.resize_image_bboxes_with_crop_or_pad(cropped_image, bboxes,300,300)\n",
    "    #dst_image, bboxes = tf_image.random_flip_left_right(cropped_image, bboxes)\n",
    "    dst_image = tf.cast(dst_image,tf.float32)\n",
    "    dst_image, bboxes = tf_image.random_flip_left_right(dst_image, bboxes)\n",
    "    dst_image = tf_image.distort_color_2(dst_image)\n",
    "    #dst_image = dst_image * 255.\n",
    "    dst_image = tf_image_whitened(dst_image, [123., 117., 104.])\n",
    "    dst_image.set_shape([300, 300, 3])\n",
    "    #dst_image = distort_color(dst_image)\n",
    "    # why take distort_color\n",
    "    image_p = tf.expand_dims(dst_image, 0)\n",
    "    image_p = tf.cast(image_p, tf.float32)\n",
    "    bboxes_p = tf.expand_dims(bboxes, 0)\n",
    "    bboxes_p = tf.maximum(bboxes_p, 0.0)\n",
    "    image_with_box = tf.image.draw_bounding_boxes(image_p, bboxes_p)\n",
    "    \n",
    "    \n",
    "    bboxes_test = tf.minimum(bboxes, 1.0)\n",
    "    # groud truth\n",
    "    glocalisations, gscores = \\\n",
    "    text_net.bboxes_encode( bboxes, text_anchors,num,match_threshold = 0.5)\n",
    "    \n",
    "    bbox_image = tf.image.draw_bounding_boxes(tf.expand_dims(dst_image,0), tf.expand_dims(bboxes,0))\n",
    "    \n",
    "    # batch\n",
    "    batch_shape = [1] + [6] * 2\n",
    "    r = tf.train.batch(\n",
    "        tf_utils.reshape_list([dst_image, glocalisations, gscores]),\n",
    "        batch_size=4,\n",
    "        num_threads=1,\n",
    "        capacity=2)\n",
    "    b_image, b_glocalisations, b_gscores= \\\n",
    "           tf_utils.reshape_list(r, batch_shape)\n",
    "     \n",
    "    inputs = b_image  \n",
    "\n",
    "    ## net predict\n",
    "    localisations, logits, end_points = \\\n",
    "    text_net.net(b_image, is_training=True)\n",
    "    \n",
    "    ## loss\n",
    "    \n",
    "    total_loss = text_net.losses(logits, localisations,\n",
    "           b_glocalisations, b_gscores,\n",
    "           match_threshold=0.5,\n",
    "           negative_ratio=3,\n",
    "           alpha=1,\n",
    "           label_smoothing=0)\n",
    "    \n",
    "    l_cross_pos = []\n",
    "    l_cross_neg = []\n",
    "    l_loc = []\n",
    "    n_poses = []\n",
    "    pmasks = []\n",
    "    for i in range(len(logits)):\n",
    "        dtype = logits[i].dtype\n",
    "        with tf.name_scope('block_%i' % i):\n",
    "\n",
    "            # Determine weights Tensor.\n",
    "            pmask = b_gscores[i] > 0.5\n",
    "            ipmask = tf.cast(pmask, tf.int32)\n",
    "            n_pos = tf.reduce_sum(ipmask)\n",
    "            fpmask = tf.cast(pmask, tf.float32)\n",
    "            nmask = b_gscores[i] < 0.5\n",
    "            inmask = tf.cast(nmask, tf.int32)\n",
    "            fnmask = tf.cast(nmask, tf.float32)\n",
    "            num = tf.ones_like(b_gscores[i])\n",
    "            n = tf.reduce_sum(num) + 1e-5\n",
    "            n_poses.append(n_pos)\n",
    "            pmasks.append(pmask)\n",
    "            # Add cross-entropy loss.\n",
    "        with tf.name_scope('cross_entropy_pos'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=ipmask)\n",
    "            #loss = tf.square(fpmask*(logits[i][:,:,:,:,:,1] - fpmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_pos.append(loss)\n",
    "        with tf.name_scope('cross_entropy_neg'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[i],labels=inmask)\n",
    "            #loss = tf.square(fnmask*(logits[i][:,:,:,:,:,0] - fnmask))\n",
    "            loss = 10*tf.reduce_mean(loss)\n",
    "            l_cross_neg.append(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "    ## Training \n",
    "\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                image_, bbox_, num_, bbox_img,image_bbox_p,l_cross_pos_,l_cross_neg_,n_poses_,loss_ = \\\n",
    "                sess.run([dst_image,bboxes,num,bbox_image,image_with_box,l_cross_pos,l_cross_neg,n_poses,total_loss])\n",
    "                box_test = sess.run([bboxes_test])\n",
    "                #print name\n",
    "                #print height,width\n",
    "                #height, width = shape[0],shape[1]\n",
    "                print loss_\n",
    "                print n_poses_\n",
    "                print l_cross_pos_, l_cross_neg_\n",
    "                print num_\n",
    "                #print len(glabels)\n",
    "                print bbox_.shape\n",
    "                print image_.shape\n",
    "\n",
    "                image_ = np.uint8(image_)*255\n",
    "                visualize_bbox(image_, bbox_)\n",
    "                \n",
    "                #bbox_img = sess.run(bbox_image)\n",
    "                #anchors_ = sess.run([text_anchors])\n",
    "                print text_anchors[0][0].shape\n",
    "                \n",
    "                glocalisations_, gscores_ = \\\n",
    "                sess.run([glocalisations, gscores])\n",
    "                print glocalisations_[0].shape\n",
    "            \n",
    "                \n",
    "                b_image_, b_glocalisations_, b_gscores_ = sess.run([b_image, b_glocalisations, b_gscores])\n",
    "                r_ = sess.run(r)\n",
    "                localisations_, logits_, end_points_ =\\\n",
    "                sess.run([localisations, logits, end_points])\n",
    "  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_s = (image_bbox_p - np.min(image_bbox_p))*255/(np.max(image_bbox_p) - np.min(image_bbox_p))\n",
    "image_s = image_s.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13405cc10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136101750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#skio.imshow(np.uint8(image_bbox_p[0,:,:,:])*255)\n",
    "skio.imshow(image_s[0,:,:,:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36474955,  0.61708784,  0.58035988,  0.9549982 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    pmask = gscores_[i] > 0.5\n",
    "    print np.sum(pmask)\n",
    "bbox_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 12, 12, 12, 12]), array([12, 13,  7,  7,  8,  8,  8,  9,  9,  9, 10, 10, 11, 11, 12, 12, 12,\n",
      "       13, 13, 13, 14, 14, 14,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n",
      "        9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 15, 15,  6,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 14,\n",
      "       14,  8,  8,  9, 13]), array([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64), array([], dtype=int64), array([], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 38, 2, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print np.where(gscores_[i] >0.1)\n",
    "gscores_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.05319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "27\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.00275449878623 6.05501136631\n"
     ]
    }
   ],
   "source": [
    "pos_loss = 0\n",
    "neg_loss = 0\n",
    "for i in range(6):\n",
    "    p_mask = np.int32(np.greater(b_gscores_[i] , 0.5))\n",
    "    print np.sum(p_mask)\n",
    "    n_mask = np.int32(np.less(b_gscores_[i] , 0.5))\n",
    "    pos_loss += np.mean(pow((p_mask * (logits_[i][:,:,:,:,:,1] - p_mask)),2))\n",
    "    neg_loss += np.mean(pow((n_mask * (logits_[i][:,:,:,:,:,0] - n_mask)),2))\n",
    "print pos_loss,neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: ../data/sythtext/*.tfrecord\n",
      "[197.63361]\n",
      "[197.36108]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # initalize the net\n",
    "    net = txtbox_300.TextboxNet()\n",
    "    out_shape = net.params.img_shape\n",
    "    anchors = net.anchors(out_shape)\n",
    "\n",
    "    # Create global_step.\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # create batch dataset\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        b_image, b_glocalisations, b_gscores = \\\n",
    "        load_batch.get_batch('../data/sythtext/',\n",
    "                             1,\n",
    "                             2,\n",
    "                             out_shape,\n",
    "                             net,\n",
    "                             anchors,\n",
    "                             1,\n",
    "                             is_training = True)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "\n",
    "        arg_scope = net.arg_scope(weight_decay=0.0005)\n",
    "\n",
    "        with slim.arg_scope(arg_scope):\n",
    "            localisations, logits, end_points = \\\n",
    "                    net.net(b_image, is_training=True)\n",
    "\n",
    "        # Add loss function.\n",
    "        total_loss = net.losses(logits, localisations,\n",
    "                           b_glocalisations, b_gscores,\n",
    "                           match_threshold=0.5,\n",
    "                           negative_ratio=3,\n",
    "                           alpha=10,\n",
    "                           label_smoothing=0.0)\n",
    "\n",
    "    # Gather summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    for end_point in end_points:\n",
    "        x = end_points[end_point]\n",
    "        summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "        summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                        tf.nn.zero_fraction(x)))\n",
    "\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for loss in tf.get_collection('EXTRA_LOSSES'):\n",
    "        summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "    for variable in slim.get_model_variables():\n",
    "        summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        #learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "          #                                               FLAGS.num_samples,\n",
    "        #                                              global_step)\n",
    "        # Configure the optimization procedure \n",
    "        #optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        #summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        ## Training \n",
    "\n",
    "        grads_and_vars = optimizer.compute_gradients(total_loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "\n",
    "    # =================================================================== #\n",
    "    # Kicks off the training.\n",
    "    # =================================================================== #\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "    config = tf.ConfigProto(log_device_placement=False,\n",
    "                            allow_soft_placement = True)\n",
    "    saver = tf.train.Saver(tf.global_variables(),\n",
    "                           max_to_keep=5,\n",
    "                           keep_checkpoint_every_n_hours=1.0,\n",
    "                           write_version=2,\n",
    "                           pad_step_number=False)\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(2):\n",
    "                loss = sess.run([total_loss])\n",
    "                if i % 1 ==0:\n",
    "                    print loss\n",
    "                #current_step = tf.train.global_step(sess, global_step)\n",
    "                '''\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print i\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
