{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import os, os.path\n",
    "import sys\n",
    "sys.path.insert(0,'../processing/')\n",
    "sys.path.insert(0,'../')\n",
    "from datasets import sythtextprovider\n",
    "import load_batch\n",
    "from nets import txtbox_300, textbox_common, np_methods\n",
    "#from processing import image_processing\n",
    "from image_processing2 import *\n",
    "from processing import txt_preprocessing\n",
    "import tf_utils\n",
    "import time\n",
    "slim = tf.contrib.slim\n",
    "import load_batch\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "from processing import tf_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    text_net = txtbox_300.TextboxNet()\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "\n",
    "    b_image, b_glocalisations, b_gscores = \\\n",
    "        load_batch.get_batch('../data/sythtext/',\n",
    "                             3,\n",
    "                             1,\n",
    "                             text_shape,\n",
    "                             text_net,\n",
    "                             text_anchors,\n",
    "                             2,\n",
    "                             file_pattern = '*.tfrecord',\n",
    "                             is_training = True,\n",
    "                             shuffe = False)\n",
    "    localisations, logits, end_points = \\\n",
    "                        text_net.net(b_image, is_training=True, use_batch=False)\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in xrange(1):\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pad2d(inputs,\n",
    "          pad=(0, 0),\n",
    "          mode='CONSTANT',\n",
    "          data_format='NHWC',\n",
    "          trainable=True,\n",
    "          scope=None):\n",
    "    \"\"\"2D Padding layer, adding a symmetric padding to H and W dimensions.\n",
    "\n",
    "    Aims to mimic padding in Caffe and MXNet, helping the port of models to\n",
    "    TensorFlow. Tries to follow the naming convention of `tf.contrib.layers`.\n",
    "\n",
    "    Args:\n",
    "      inputs: 4D input Tensor;\n",
    "      pad: 2-Tuple with padding values for H and W dimensions;\n",
    "      mode: Padding mode. C.f. `tf.pad`\n",
    "      data_format:  NHWC or NCHW data format.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'pad2d', [inputs]):\n",
    "        # Padding shape.\n",
    "        if data_format == 'NHWC':\n",
    "            paddings = [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]]\n",
    "        elif data_format == 'NCHW':\n",
    "            paddings = [[0, 0], [0, 0], [pad[0], pad[0]], [pad[1], pad[1]]]\n",
    "        net = tf.pad(inputs, paddings, mode=mode)\n",
    "        return net\n",
    "    \n",
    "def conv2d(inputs, out, kernel_size, scope,stride=1,activation_fn=tf.nn.relu, \n",
    "\t\t\tpadding = 'SAME', use_batch=False, batch_norm_params={}, rate = 1):\n",
    "\tif use_batch:\n",
    "\t\tnet = slim.conv2d(inputs, out, kernel_size, stride=stride ,scope=scope, normalizer_fn=slim.batch_norm, \n",
    "\t\t\t  normalizer_params=batch_norm_params, activation_fn=activation_fn ,padding = padding, rate = rate)\n",
    "\telse:\n",
    "\t\tnet = slim.conv2d(inputs, out, kernel_size, stride=stride, scope=scope, activation_fn=activation_fn,padding = padding, rate = rate)\n",
    "\treturn net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scope='text_box_300'\n",
    "inputs = tf.placeholder(shape=(None,512,512,3),dtype=tf.float32)\n",
    "use_batch = True\n",
    "is_training= True\n",
    "batch_norm_params = {\n",
    "  # Decay for the moving averages.\n",
    "  'decay': 0.9997,\n",
    "  # epsilon to prevent 0s in variance.\n",
    "  'epsilon': 0.001,\n",
    "  'is_training': is_training,\n",
    "}\n",
    "end_points = {}\n",
    "with tf.variable_scope(scope, 'text_box_300', [inputs], reuse=True):\n",
    "    # Original VGG-16 blocks.\n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "    end_points['conv1'] = net\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "    # Block 2.\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "    end_points['conv2'] = net # 150,150 128\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "    # Block 3. # 75 75 256\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "    end_points['conv3'] = net\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool3',padding='SAME')\n",
    "    # Block 4. # 38 38 512\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "    end_points['conv4'] = net\n",
    "    #net = slim.max_pool2d(net, [2, 2],scope='pool4')\n",
    "    net = slim.max_pool2d(net, [2, 2], stride=2, scope='pool4')\n",
    "    # Block 5. # 19 19 512\n",
    "    #net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "    end_points['conv5'] = net\n",
    "    net = slim.max_pool2d(net, [3, 3], stride=1, scope='pool5',padding='SAME')\n",
    "\n",
    "    # Additional SSD blocks.\n",
    "    # Block 6: let's dilate the hell out of it!\n",
    "    #net = slim.conv2d(net, 1024, [3, 3], scope='conv6')\n",
    "    #net = conv2d(net, 1024, [3,3], scope='conv6',rate=1, use_batch=use_batch, batch_norm_params= batch_norm_params)\n",
    "    net = conv2d(net, 1024, [3,3], scope='conv6',rate=6, use_batch=use_batch, batch_norm_params= batch_norm_params)\n",
    "    end_points['conv6'] = net\n",
    "    # Block 7: 1x1 conv. Because the fuck.\n",
    "    #net = slim.conv2d(net, 1024, [1, 1], scope='conv7')\n",
    "    net = conv2d(net, 1024, [1, 1], scope='conv7',use_batch=use_batch, batch_norm_params= batch_norm_params)\n",
    "    end_points['conv7'] = net\n",
    "    # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).\n",
    "    end_point = 'conv8'\n",
    "    with tf.variable_scope(end_point):\n",
    "        #net = slim.conv2d(net, 256, [1, 1], scope='conv1x1')\n",
    "        #net = slim.conv2d(net, 512, [3, 3], stride=2, scope='conv3x3')\n",
    "        net = conv2d(net, 256, [1, 1], scope='conv1x1',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "        net = conv2d(net, 512, [3, 3], stride=2, scope='conv3x3',use_batch=use_batch, batch_norm_params=batch_norm_params)\t\n",
    "    end_points[end_point] = net\n",
    "    end_point = 'conv9'\n",
    "    with tf.variable_scope(end_point):\n",
    "        net = conv2d(net, 128, [1, 1], scope='conv1x1', use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "        net = conv2d(net, 256, [3, 3], stride=2, scope='conv3x3',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "    end_points[end_point] = net\n",
    "    end_point = 'conv10'\n",
    "    with tf.variable_scope(end_point):\n",
    "        net = conv2d(net, 128, [1, 1], scope='conv1x1',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "        net = conv2d(net, 256, [3, 3], stride=2, scope='conv3x3',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "    end_points[end_point] = net\n",
    "    end_point = 'global'\n",
    "    with tf.variable_scope(end_point):\n",
    "        net = conv2d(net, 128, [1, 1], scope='conv1x1',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "        net = conv2d(net, 256, [3, 3], scope='conv3x3', padding='VALID',use_batch=use_batch, batch_norm_params=batch_norm_params)\n",
    "    end_points[end_point] = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': <tf.Tensor 'text_box_300_7/conv1/conv1_2/Relu:0' shape=(?, 512, 512, 64) dtype=float32>,\n",
       " 'conv10': <tf.Tensor 'text_box_300_7/conv10/conv3x3/Relu:0' shape=(?, 4, 4, 256) dtype=float32>,\n",
       " 'conv2': <tf.Tensor 'text_box_300_7/conv2/conv2_2/Relu:0' shape=(?, 256, 256, 128) dtype=float32>,\n",
       " 'conv3': <tf.Tensor 'text_box_300_7/conv3/conv3_3/Relu:0' shape=(?, 128, 128, 256) dtype=float32>,\n",
       " 'conv4': <tf.Tensor 'text_box_300_7/conv4/conv4_3/Relu:0' shape=(?, 64, 64, 512) dtype=float32>,\n",
       " 'conv5': <tf.Tensor 'text_box_300_7/conv5/conv5_3/Relu:0' shape=(?, 32, 32, 512) dtype=float32>,\n",
       " 'conv6': <tf.Tensor 'text_box_300_7/conv6/Relu:0' shape=(?, 32, 32, 1024) dtype=float32>,\n",
       " 'conv7': <tf.Tensor 'text_box_300_7/conv7/Relu:0' shape=(?, 32, 32, 1024) dtype=float32>,\n",
       " 'conv8': <tf.Tensor 'text_box_300_7/conv8/conv3x3/Relu:0' shape=(?, 16, 16, 512) dtype=float32>,\n",
       " 'conv9': <tf.Tensor 'text_box_300_7/conv9/conv3x3/Relu:0' shape=(?, 8, 8, 256) dtype=float32>,\n",
       " 'global': <tf.Tensor 'text_box_300_7/global/conv3x3/Relu:0' shape=(?, 2, 2, 256) dtype=float32>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2.7",
   "language": "python",
   "name": "tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
