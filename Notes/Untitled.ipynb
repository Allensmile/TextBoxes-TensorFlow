{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import os, os.path\n",
    "import sys\n",
    "sys.path.insert(0,'../processing/')\n",
    "sys.path.insert(0,'../')\n",
    "from datasets import sythtextprovider\n",
    "from nets import txtbox_300, textbox_common, np_methods\n",
    "#from processing import image_processing\n",
    "from image_processing2 import *\n",
    "from processing import ssd_vgg_preprocessing, visualization,txt_preprocessing\n",
    "import tf_utils\n",
    "import time\n",
    "slim = tf.contrib.slim\n",
    "import load_batch\n",
    "import numpy as np\n",
    "import skimage.io as skio\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchor_sizes=[(30., 60.),\n",
    "          (60., 114.),\n",
    "          (114., 168.),\n",
    "          (168., 222.),\n",
    "          (222., 276.),\n",
    "          (276., 330.)]\n",
    "anchor_sizes=[(21., 45.),\n",
    "              (45., 99.),\n",
    "              (99., 153.),\n",
    "              (153., 207.),\n",
    "              (207., 261.),\n",
    "              (261., 315.)]\n",
    "scale_range=[0.15, 0.9]\n",
    "scale_range_max = [0.2, 1.1]\n",
    "scales = [scale_range[0] + i*(scale_range[1] - scale_range[0])/5  for i in range(6)]\n",
    "scales_max = [scale_range_max[0] + i*(scale_range_max[1] - scale_range_max[0])/5  for i in range(6)]\n",
    "anchor_sizes = [(300*scales[i], 300*scales_max[i]) for i in range(6)]\n",
    "anchor_sizes\n",
    "anchor_sizes=[(30., 60.),\n",
    "          (60., 114.),\n",
    "          (114., 168.),\n",
    "          (168., 222.),\n",
    "          (222., 276.),\n",
    "          (276., 330.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_shape (300, 300)\n",
      "0.5\n",
      "file_path: ../data/ICDAR2013/*.tfrecord\n",
      "68\n",
      "7391\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    # build a net\\\n",
    "    params = txtbox_300.TextboxNet.default_params\n",
    "    params = params._replace(anchor_sizes = anchor_sizes)\n",
    "    text_net = txtbox_300.TextboxNet(params)\n",
    "    text_shape = text_net.params.img_shape\n",
    "    print 'text_shape '+  str(text_shape)\n",
    "    text_anchors = text_net.anchors(text_shape)\n",
    "    print text_net.params.match_threshold\n",
    "    \n",
    "    ## dataset provider\n",
    "    dataset = sythtextprovider.get_datasets('../data/ICDAR2013/',file_pattern='*.tfrecord')\n",
    "    \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset, common_queue_capacity=32, common_queue_min=2)\n",
    "    \n",
    "    [image, shape, glabels, gbboxes] = \\\n",
    "    data_provider.get(['image', 'shape',\n",
    "                     'object/label',\n",
    "                     'object/bbox'])\n",
    "    \n",
    "    dst_image, glabels, gbboxes,num = \\\n",
    "    txt_preprocessing.preprocess_image(image,  glabels,gbboxes, \n",
    "                                            text_shape,is_training=True)\n",
    "\n",
    "    glocalisations, gscores = \\\n",
    "    text_net.bboxes_encode( gbboxes, text_anchors, num)\n",
    "    for i in range(6):\n",
    "        glocalisations[i] = tf.expand_dims(glocalisations[i], axis=0)\n",
    "        gscores[i] = tf.expand_dims(gscores[i], axis=0)\n",
    "    \n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            error = []\n",
    "            box = []\n",
    "            for i in xrange(229):\n",
    "                rpredictions, rlocalisations, img ,gbboxes_= sess.run([gscores, glocalisations,dst_image,gbboxes])\n",
    "                rpredictions_2 = list(rpredictions)\n",
    "                localb = []\n",
    "                for i in range(6):\n",
    "                    decodeb = np_methods.ssd_bboxes_decode(rlocalisations[i],text_anchors[i])\n",
    "                    localb.append(decodeb[np.where(rpredictions[i] > 0.5)])\n",
    "                    pre2 = np.expand_dims(1-rpredictions[i], -1)\n",
    "                    rpredictions[i] = np.concatenate([pre2, np.expand_dims(rpredictions[i], -1)],axis = -1)\n",
    "                rclasses, rscores, rbboxes = np_methods.ssd_bboxes_select(\n",
    "                        rpredictions, rlocalisations, text_anchors,\n",
    "                        select_threshold=0.001, img_shape=text_shape, num_classes=2, decode=True)\n",
    "\n",
    "                rbboxes = np_methods.bboxes_clip(rbboxes)\n",
    "                rclasses, rscores, rbboxes = np_methods.bboxes_sort(rclasses, rscores, rbboxes, top_k=-1)\n",
    "                rclasses, rscores, rbboxes = np_methods.bboxes_nms(rclasses, rscores, rbboxes, \n",
    "                                                                  nms_threshold=0.45)\n",
    "                #Resize bboxes to original image shape. Note: useless for Resize.WARP!\n",
    "                bboxes = np.concatenate(localb, 0)\n",
    "                           \n",
    "                image_ = np.uint8(img)*255\n",
    "                img = image_.copy()\n",
    "                #visualize_bbox(img, rbboxes)\n",
    "                \n",
    "                #img = image_.copy()\n",
    "                #visualize_bbox(img, bboxes)\n",
    "                img = image_.copy()\n",
    "                #visualize_bbox(img, gbboxes_)\n",
    "                \n",
    "                img = image_.copy()\n",
    "\n",
    "                \n",
    "                for i in range(6):\n",
    "                    #pass\n",
    "                    box.append(rlocalisations[i][np.where(rpredictions_2[i] > 0.5)].shape[0])\n",
    "                #error.append((gbboxes_.shape[0] - rbboxes.shape[0]))\n",
    "                error.append((gbboxes_.shape[0] - rbboxes.shape[0]))\n",
    "            print sum(error)\n",
    "            print sum(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4)\n",
      "(6, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print rbboxes.shape\n",
    "print gbboxes_.shape\n",
    "#\n",
    "np_methods.bboxes_clip(gbboxes_)\n",
    "rbboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 4)\n",
      "(8, 4)\n",
      "(15, 4)\n",
      "(2, 4)\n",
      "(0, 4)\n",
      "(0, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print rlocalisations[i][np.where(rpredictions_2[i] > 0.5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bboxes):\n",
    "    \"\"\"\n",
    "    Input: image (height, width, channels)\n",
    "           bboxes (numof bboxes, 4) in order(ymin, xmin, ymax, xmax)\n",
    "                  range(0,1) \n",
    "    \"\"\"\n",
    "    numofbox = bboxes.shape[0]\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "    def norm(x):\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        else:\n",
    "            if x > 1:\n",
    "                x = 1\n",
    "        return x\n",
    "    xmin = [int(i * width) for i in bboxes[:,1]]\n",
    "    ymin = [int(i * height) for i in bboxes[:,0]]\n",
    "    ymax = [int(i * height) for i in bboxes[:,2]]\n",
    "    xmax = [int(i * width) for i in bboxes[:,3]]\n",
    "\n",
    "    for i in range(numofbox):\n",
    "        image = cv2.rectangle(image,(xmin[i],ymin[i]),\n",
    "                             (xmax[i],ymax[i]),(0,255,255))\n",
    "    #print [ymin,xmin,ymax,xmax]\n",
    "    skio.imshow(image)\n",
    "    skio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "min_dim = 300\n",
    "min_ratio = 20\n",
    "max_ratio = 95\n",
    "step = int(math.floor((max_ratio - min_ratio) / (6 - 2)))\n",
    "min_sizes = []\n",
    "max_sizes = []\n",
    "for ratio in xrange(min_ratio, max_ratio + 1, step):\n",
    "    min_sizes.append(min_dim * ratio / 100.)\n",
    "    max_sizes.append(min_dim * (ratio + step) / 100.)\n",
    "min_sizes = [min_dim * 10 / 100.] + min_sizes\n",
    "max_sizes = [[]] + max_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(fname='result.csv',X=a,delimiter=',',fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], 114.0, 168.0, 222.0, 276.0, 330.0]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30.0, 60.0, 114.0, 168.0, 222.0, 276.0]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tf_2.7",
   "language": "python",
   "name": "tensorflow2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
